{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVfMmn30u6gb"
      },
      "source": [
        "# Graph-Based Audio-Visual Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bag8IGwXvNrn"
      },
      "source": [
        "CS224W (23/24 Fall) Project from Zhengyang Wei, Tianyuan Dai, & Haoyi Duan"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before you get started:**\n",
        "- This Colab includes our PyG implementation of the paper ***Graph-Based Video-Language Learning with Multi-Grained\n",
        "Audio-Visual Alignment***. The paper author hasn't shared any open-source code, so we've implemented the code ourselves.\n",
        "\n",
        "    - Link to the pdf of this paper: https://dl.acm.org/doi/pdf/10.1145/3581783.3612132\n",
        "\n",
        "- Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell.\n",
        "- The data is stored in [Google Drive](https://drive.google.com/drive/folders/175T6bEFoC2X8qww7wfuxQu_yMSpik-yS?usp=drive_link). Feel free to make a copy to your drive!"
      ],
      "metadata": {
        "id": "EfrWcpzvQQbh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O5VqTprw309"
      },
      "source": [
        "## 1. Colab Tutorial Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN3foK30vbJq"
      },
      "source": [
        "*Mount Google Drive for Loading Subset of MUSIC-AVQA dataset.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEqzk5E9v63Q",
        "outputId": "53b958d9-7c5b-48a5-c036-0ede94c29e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXM0WC1w-68"
      },
      "source": [
        "## 2. Environment Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQrtNU2IxF6m"
      },
      "source": [
        "### 2.1 Install Dependencis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m55Xm_eDwG8o",
        "outputId": "5756f7aa-7b76-43fa-c3e5-3ad6a862a501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.42.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n",
            "Collecting torchaudio==0.13.0\n",
            "  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio==0.13.0) (1.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchaudio==0.13.0) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchaudio==0.13.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchaudio==0.13.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchaudio==0.13.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchaudio==0.13.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchaudio==0.13.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchaudio==0.13.0) (0.42.0)\n",
            "Installing collected packages: torchaudio\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.1.0+cu118\n",
            "    Uninstalling torchaudio-2.1.0+cu118:\n",
            "      Successfully uninstalled torchaudio-2.1.0+cu118\n",
            "Successfully installed torchaudio-0.13.0\n",
            "Collecting torchvision==0.14.0\n",
            "  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (1.13.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (9.4.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchvision==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision==0.14.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision==0.14.0) (0.42.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (2023.11.17)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "Successfully installed torchvision-0.14.0\n",
            "Collecting ffmpeg==1.4\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6080 sha256=8b52a04e04ef25a8528247041a9acc66ba8c519b3d1fe901555cf3f4c986c954\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Collecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "flax 0.7.5 requires numpy>=1.22, but you have numpy 1.21.5 which is incompatible.\n",
            "jax 0.4.20 requires numpy>=1.22, but you have numpy 1.21.5 which is incompatible.\n",
            "jaxlib 0.4.20+cuda11.cudnn86 requires numpy>=1.22, but you have numpy 1.21.5 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 1.21.5 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.21.5 which is incompatible.\n",
            "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 1.21.5 which is incompatible.\n",
            "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 1.21.5 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.21.5 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Collecting SceneGraphParser\n",
            "  Downloading SceneGraphParser-0.1.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: spacy>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from SceneGraphParser) (3.6.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from SceneGraphParser) (0.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.0->SceneGraphParser) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.0->SceneGraphParser) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->SceneGraphParser) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->SceneGraphParser) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->SceneGraphParser) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->SceneGraphParser) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.0->SceneGraphParser) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.0->SceneGraphParser) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.0->SceneGraphParser) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.0->SceneGraphParser) (2.1.3)\n",
            "Installing collected packages: SceneGraphParser\n",
            "Successfully installed SceneGraphParser-0.1.0\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.3\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.0\n",
        "!pip install torchaudio==0.13.0\n",
        "!pip install torchvision==0.14.0\n",
        "!pip install ffmpeg==1.4\n",
        "!pip install numpy==1.21.5\n",
        "!pip install tensorboardX\n",
        "!pip install spacy\n",
        "!pip install SceneGraphParser\n",
        "!pip install ftfy\n",
        "!pip install regex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qJBmgZyyUIj"
      },
      "source": [
        "### 2.2 Install Pytorch Geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_---h8Emyeaz"
      },
      "source": [
        "- This verifies the torch and cuda version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIur1uTKyFxJ",
        "outputId": "605bc3f7-8995-4b37-9c69-0231aaf45e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  1.13 ; cuda:  cu117\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeBZfxQUzTzF"
      },
      "source": [
        "- This installs the Pytorch Geometric library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ofbxXYYzQZa",
        "outputId": "6a3d9b4b-680d-4fc2-a001-3313c1b9e578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3522473 sha256=ffadc85ab6e802b8581141378dcfc923c8dd6676897dfd1fa1cf79e9ef621881\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu117.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Collecting numpy<1.28.0,>=1.21.6 (from scipy->torch-sparse)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=2660199 sha256=c6443344b13784239201f923bfcaa5d14786eaba6bd90f78a0a11b11e2bb9790\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: numpy, torch-sparse\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.2 torch-sparse-0.6.18\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "import os\n",
        "import torch\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JkFYFgx0zoxU",
        "outputId": "9e2d6f6b-4c1f-4eae-997e-5cd3ab245ffe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-1kAqf7JGq"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "*You can **skip** this whole section since the processed data is already stored in [Google Drive](https://drive.google.com/drive/folders/175T6bEFoC2X8qww7wfuxQu_yMSpik-yS?usp=drive_link).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEbZarYHh7Hl"
      },
      "source": [
        "- Load clip model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUc8SvAzgpsV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path = [\"/content/drive/MyDrive/AVQA-GNN\"] + sys.path\n",
        "import clip_net.clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip_net.clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4500WAwhqHg"
      },
      "source": [
        "### 3.1 Question Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsJtBIsi7SW5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "import json\n",
        "import ast\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "def qst_feat_extract(qst):\n",
        "\n",
        "    text = clip_net.clip.tokenize(qst).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    return text_features\n",
        "\n",
        "\n",
        "def QstCLIP_feat(json_path, dst_qst_path):\n",
        "\n",
        "    samples = json.load(open(json_path, 'r'))\n",
        "\n",
        "    ques_vocab = ['<pad>']\n",
        "\n",
        "    i = 0\n",
        "    for sample in tqdm(samples):\n",
        "        i += 1\n",
        "        question = sample['question_content'].rstrip().split(' ')\n",
        "        question[-1] = question[-1][:-1]\n",
        "\n",
        "        question_id = sample['question_id']\n",
        "        # print(\"\\n\")\n",
        "        # print(\"question id: \", question_id)\n",
        "\n",
        "        save_file = os.path.join(dst_qst_path, str(question_id) + '.npy')\n",
        "\n",
        "        if os.path.exists(save_file):\n",
        "            print(question_id, \" is already exist!\")\n",
        "            continue\n",
        "\n",
        "        p = 0\n",
        "        for pos in range(len(question)):\n",
        "            if '<' in question[pos]:\n",
        "                question[pos] = ast.literal_eval(sample['templ_values'])[p]\n",
        "                p += 1\n",
        "        for wd in question:\n",
        "            if wd not in ques_vocab:\n",
        "                ques_vocab.append(wd)\n",
        "\n",
        "        question = ' '.join(question)\n",
        "\n",
        "        qst_feat = qst_feat_extract(question)\n",
        "\n",
        "        qst_features = qst_feat.float().cpu().numpy()\n",
        "        # print(qst_features.shape)\n",
        "        np.save(save_file, qst_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r4pVWTpiH1t",
        "outputId": "66bd3dd5-9adc-4dc6-9b10-dde68d33348e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45624/45624 [15:32<00:00, 48.94it/s]\n"
          ]
        }
      ],
      "source": [
        "json_path = \"/content/drive/MyDrive/AVQA-GNN/dataset/split_que_id/music_avqa.json\"\n",
        "dst_qst_path = \"/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA/clip_qst/\"\n",
        "os.makedirs(dst_qst_path, exist_ok=True)\n",
        "\n",
        "QstCLIP_feat(json_path, dst_qst_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTBwr6TBu88F"
      },
      "source": [
        "### 3.2 Visual Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRe59Ef5vOZW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "def clip_feat_extract(img):\n",
        "\n",
        "    image = preprocess(Image.open(img)).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "    return image_features\n",
        "\n",
        "\n",
        "def ImageClIP_Patch_feat_extract(dir_fps_path, dst_clip_path):\n",
        "\n",
        "    video_list = os.listdir(dir_fps_path)\n",
        "    video_idx = 0\n",
        "    total_nums = len(video_list)\n",
        "\n",
        "    for video in video_list:\n",
        "\n",
        "        video_idx = video_idx + 1\n",
        "        print(\"\\n--> \", video_idx, video)\n",
        "\n",
        "        save_file = os.path.join(dst_clip_path, video + '.npy')\n",
        "        if os.path.exists(save_file):\n",
        "            print(video + '.npy', \"is already processed!\")\n",
        "            continue\n",
        "\n",
        "        video_img_list = sorted(glob.glob(os.path.join(dir_fps_path, video, '*.jpg')))\n",
        "\n",
        "        params_frames = len(video_img_list)\n",
        "        samples = np.round(np.linspace(0, params_frames-1, params_frames))\n",
        "\n",
        "        img_list  = [video_img_list[int(sample)] for sample in samples]\n",
        "        img_features = torch.zeros(len(img_list), 50, 512)\n",
        "\n",
        "        idx = 0\n",
        "        for img_cont in img_list:\n",
        "            img_idx_feat = clip_feat_extract(img_cont)\n",
        "            img_features[idx] = img_idx_feat\n",
        "            idx += 1\n",
        "\n",
        "        img_features = img_features.float().cpu().numpy()\n",
        "        np.save(save_file, img_features)\n",
        "\n",
        "        print(\"Process: \", video_idx, \" / \", total_nums, \" ----- video id: \", video_idx, \" ----- save shape: \", img_features.shape)\n",
        "\n",
        "\n",
        "def ImageClIP_feat_extract(dir_fps_path, dst_clip_path):\n",
        "\n",
        "    video_list = os.listdir(dir_fps_path)\n",
        "    video_idx = 0\n",
        "    total_nums = len(video_list)\n",
        "\n",
        "    for video in video_list:\n",
        "\n",
        "        video_idx = video_idx + 1\n",
        "        print(\"\\n--> \", video_idx, video)\n",
        "\n",
        "        save_file = os.path.join(dst_clip_path, video + '.npy')\n",
        "        if os.path.exists(save_file):\n",
        "            print(video + '.npy', \"is already processed!\")\n",
        "            continue\n",
        "\n",
        "        video_img_list = sorted(glob.glob(os.path.join(dir_fps_path, video, '*.jpg')))\n",
        "\n",
        "        params_frames = len(video_img_list)\n",
        "        samples = np.round(np.linspace(0, params_frames-1, params_frames))\n",
        "\n",
        "        img_list  = [video_img_list[int(sample)] for sample in samples]\n",
        "        img_features = torch.zeros(len(img_list), 512)\n",
        "\n",
        "        idx = 0\n",
        "        for img_cont in img_list:\n",
        "            img_idx_feat = clip_feat_extract(img_cont)\n",
        "            img_features[idx] = img_idx_feat\n",
        "            idx += 1\n",
        "\n",
        "        img_features = img_features.float().cpu().numpy()\n",
        "        np.save(save_file, img_features)\n",
        "\n",
        "        print(\"Process: \", video_idx, \" / \", total_nums, \" ----- video id: \", video_idx, \" ----- save shape: \", img_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdc40AWAvyM3"
      },
      "outputs": [],
      "source": [
        "dir_fps_path = '/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA/frames'\n",
        "dst_clip_path = '/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA/clip_vit_b32'\n",
        "os.makedirs(dst_clip_path, exist_ok=True)\n",
        "\n",
        "ImageClIP_feat_extract(dir_fps_path, dst_clip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia5DSgrlxd4G"
      },
      "source": [
        "### 3.3 Scene Graph Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KlSnkHAxuzC"
      },
      "outputs": [],
      "source": [
        "!pip install sng_parser\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBAOfJRz34QH"
      },
      "outputs": [],
      "source": [
        "API_TOKEN = \"Your Hugging-face API token\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "\n",
        "def text_encoder(text):\n",
        "    text = clip_net.clip.tokenize(text).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    return text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li2Xgs56xiBN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import glob\n",
        "import numpy as np\n",
        "import sng_parser\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "def query(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.post(API_URL, headers=headers, data=data)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def scene_graph_parsing(dir_fps_path, dst_scenegraph_path):\n",
        "\n",
        "    video_list = os.listdir(dir_fps_path)\n",
        "    video_idx = 0\n",
        "    total_nums = len(video_list)\n",
        "\n",
        "    scene_graphs = {}\n",
        "    name2idx = {}\n",
        "    id = 0\n",
        "    for video in video_list:\n",
        "\n",
        "        video_idx = video_idx + 1\n",
        "        print(\"\\n--> \", video_idx, video)\n",
        "\n",
        "        video_img_list = sorted(glob.glob(os.path.join(dir_fps_path, video, '*.jpg')))\n",
        "\n",
        "        params_frames = len(video_img_list)\n",
        "        samples = np.round(np.linspace(0, params_frames-1, params_frames))\n",
        "\n",
        "        img_list = [video_img_list[int(sample)] for sample in samples]\n",
        "        img_captions = []\n",
        "\n",
        "        for img_count in img_list:\n",
        "            while True:\n",
        "                try:\n",
        "                    output = query(img_count)\n",
        "                    _ = output[0]['generated_text']\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(output, str(e))\n",
        "            img_caption = output[0]['generated_text']\n",
        "            img_captions.append(img_caption)\n",
        "\n",
        "        scene_graph = {}\n",
        "        for caption_id, img_caption in enumerate(img_captions):\n",
        "            graph = sng_parser.parse(img_caption)\n",
        "            scene_graph[caption_id] = graph\n",
        "\n",
        "        data_new = {}\n",
        "        for key, value in scene_graph.items():\n",
        "            for i, entity in enumerate(value['entities']):\n",
        "                value['entities'][i]['span_embedding'] = text_encoder(entity['span'])\n",
        "            for i, relation in enumerate(value['relations']):\n",
        "                value['relations'][i]['relation_embedding'] = text_encoder(relation['relation'])\n",
        "            data_new[int(key)] = value\n",
        "\n",
        "        name = video\n",
        "        scene_graphs[id] = data_new\n",
        "        name2idx[name] = id\n",
        "        id += 1\n",
        "\n",
        "    np.save(os.path.join(dst_scenegraph_path, 'scene_graphs.npy'), scene_graphs)\n",
        "    with open(os.path.join(dst_scenegraph_path, 'name2idx.json'), 'w') as file:\n",
        "        json.dump(name2idx, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIk3c1h41dlN"
      },
      "outputs": [],
      "source": [
        "dir_fps_path = \"/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA/frames\"\n",
        "dst_scenegraph_path = \"/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA/scene_graphs_npy\"\n",
        "os.makedirs(dst_scenegraph_path, exist_ok=True)\n",
        "\n",
        "scene_graph_parsing(dir_fps_path, dst_scenegraph_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAoIenhBxirH"
      },
      "source": [
        "### 3.4 Query Graph Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh-D7gDAxmka"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import sng_parser\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "def query(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.post(API_URL, headers=headers, data=data)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def query_graph_parsing(json_path, dst_qst_path):\n",
        "\n",
        "    samples = json.load(open(json_path, 'r'))\n",
        "\n",
        "    ques_vocab = ['<pad>']\n",
        "\n",
        "    query_graphs = {}\n",
        "    name2idx = {}\n",
        "    id = 0\n",
        "    i = 0\n",
        "    for sample in tqdm(samples):\n",
        "        i += 1\n",
        "        question = sample['question_content'].rstrip().split(' ')\n",
        "        question[-1] = question[-1][:-1]\n",
        "\n",
        "        question_id = sample['question_id']\n",
        "\n",
        "        p = 0\n",
        "        for pos in range(len(question)):\n",
        "            if '<' in question[pos]:\n",
        "                question[pos] = ast.literal_eval(sample['templ_values'])[p]\n",
        "                p += 1\n",
        "        for wd in question:\n",
        "            if wd not in ques_vocab:\n",
        "                ques_vocab.append(wd)\n",
        "\n",
        "        question = ' '.join(question)\n",
        "\n",
        "        # parsing\n",
        "        data = sng_parser.parse(question)\n",
        "\n",
        "        for i, entity in enumerate(data['entities']):\n",
        "            data['entities'][i]['span_embedding'] = text_encoder(entity['span'])\n",
        "        for i, relation in enumerate(data['relations']):\n",
        "            data['relations'][i]['relation_embedding'] = text_encoder(relation['relation'])\n",
        "\n",
        "        name = str(question_id)\n",
        "        query_graphs[id] = data\n",
        "        name2idx[name] = id\n",
        "        id += 1\n",
        "\n",
        "    np.save(os.path.join(dst_qst_path, 'query_graphs.npy'), query_graphs)\n",
        "    with open(os.path.join(dst_qst_path, 'name2idx.json'), 'w') as file:\n",
        "        json.dump(name2idx, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shiPiw4b3-gi",
        "outputId": "d8fc7ec0-ce14-485b-fc2c-4e2e04fb1f9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45624/45624 [40:48<00:00, 18.63it/s]\n"
          ]
        }
      ],
      "source": [
        "json_path = \"/content/drive/MyDrive/AVQA-GNN/dataset/split_que_id/music_avqa.json\"\n",
        "dst_qst_path = \"/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA/query_graphs_npy\"\n",
        "os.makedirs(dst_qst_path, exist_ok=True)\n",
        "\n",
        "query_graph_parsing(json_path, dst_qst_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tamh3BZ43WZ"
      },
      "source": [
        "## 4. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unphYuUN5CjH"
      },
      "source": [
        "### 4.1 GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B31Kx0QD5GNz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import softmax\n",
        "import torch_scatter\n",
        "from typing import Union, Tuple, Optional\n",
        "\n",
        "class GAT(MessagePassing):\n",
        "    def __init__(self,\n",
        "                 in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int,\n",
        "                 edge_in_channels: int,\n",
        "                 heads: int = 1,\n",
        "                 negative_slope: float = 0.2,\n",
        "                 dropout: float = 0.0,\n",
        "                 add_self_loops: bool = True,\n",
        "                 **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            self.lin_l = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
        "            self.lin_r = self.lin_l\n",
        "        else:\n",
        "            self.lin_l = nn.Linear(in_channels[0], heads * out_channels, bias=False)\n",
        "            self.lin_r = nn.Linear(in_channels[1], heads * out_channels, bias=False)\n",
        "\n",
        "        self.att_l = Parameter(torch.zeros(heads, out_channels))\n",
        "        self.att_r = Parameter(torch.zeros(heads, out_channels))\n",
        "\n",
        "        self.lin_e = nn.Linear(edge_in_channels, heads * out_channels, bias=False)\n",
        "        self.att_e = Parameter(torch.zeros(heads, out_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_e.weight)\n",
        "        nn.init.xavier_uniform_(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "        nn.init.xavier_uniform_(self.att_e)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, size = None):\n",
        "\n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        x_l = self.lin_l(x).view(-1, H, C)\n",
        "        x_r = self.lin_r(x).view(-1, H, C)\n",
        "        alpha_l = self.att_l.unsqueeze(0) * x_l\n",
        "        alpha_r = self.att_r.unsqueeze(0) * x_r\n",
        "\n",
        "        e = self.lin_e(edge_attr).view(-1, H, C)\n",
        "        alpha_e = self.att_e.unsqueeze(0) * e\n",
        "\n",
        "        out = self.propagate(edge_index=edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), alpha_e=alpha_e, size=size).view(-1, H*C)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, alpha_j, alpha_i, alpha_e, index, ptr, size_i):\n",
        "\n",
        "        alpha = alpha_i + alpha_j + alpha_e\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)\n",
        "        alpha = F.dropout(alpha, p=self.dropout)\n",
        "        out = alpha * x_j\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class GNNStack(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, edge_attr_dim, num_layers, heads=4, dropout=0., negative_slope=0.2):\n",
        "        super(GNNStack, self).__init__()\n",
        "\n",
        "        assert (num_layers >= 1), 'Number of layers is not >= 1'\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GAT(in_channels=in_channels, out_channels=out_channels,\n",
        "                    edge_in_channels=edge_attr_dim, heads=heads, negative_slope=negative_slope, dropout=dropout))\n",
        "        assert num_layers >= 1, 'Number of layers is not >= 1'\n",
        "        for l in range(num_layers - 1):\n",
        "            self.convs.append(GAT(in_channels=heads * in_channels, out_channels=out_channels,\n",
        "                    edge_in_channels=edge_attr_dim, heads=heads, negative_slope=negative_slope, dropout=dropout))\n",
        "\n",
        "        self.bns = nn.ModuleList([nn.BatchNorm1d(heads * out_channels) for _ in range(num_layers - 1)])\n",
        "\n",
        "        self.post_mp = nn.Sequential(\n",
        "                    nn.Linear(heads * in_channels, out_channels), nn.Dropout(dropout),\n",
        "                    nn.Linear(out_channels, out_channels))\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, edge_attr)\n",
        "            if i != self.num_layers - 1:\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGCOmZ3x5D3z"
      },
      "source": [
        "### 4.2 LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-X7HGt95Gr8"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.typing import OptTensor\n",
        "\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "from torch import Tensor\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "from torch_geometric.nn.inits import ones, zeros\n",
        "\n",
        "class LayerNorm(torch.nn.Module):\n",
        "    r\"\"\"Applies layer normalization over each individual example in a batch\n",
        "    of node features as described in the `\"Instance Normalization: The Missing\n",
        "    Ingredient for Fast Stylization\" <https://arxiv.org/abs/1607.08022>`_\n",
        "    paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{x}^{\\prime}_i = \\frac{\\mathbf{x} -\n",
        "        \\textrm{E}[\\mathbf{x}]}{\\sqrt{\\textrm{Var}[\\mathbf{x}] + \\epsilon}}\n",
        "        \\odot \\gamma + \\beta\n",
        "\n",
        "    The mean and standard-deviation are calculated across all nodes and all\n",
        "    node channels separately for each object in a mini-batch.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        eps (float, optional): A value added to the denominator for numerical\n",
        "            stability. (default: :obj:`1e-5`)\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, eps=1e-5, affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.eps = eps\n",
        "\n",
        "        if affine:\n",
        "            self.weight = Parameter(torch.Tensor([in_channels]))\n",
        "            self.bias = Parameter(torch.Tensor([in_channels]))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        ones(self.weight)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    def forward(self, x: Tensor, batch: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if batch is None:\n",
        "            x = x - x.mean()\n",
        "            out = x / (x.std(unbiased=False) + self.eps)\n",
        "\n",
        "        else:\n",
        "            batch_size = int(batch.max()) + 1\n",
        "\n",
        "            norm = degree(batch, batch_size, dtype=x.dtype).clamp_(min=1)\n",
        "            norm = norm.mul_(x.size(-1)).view(-1, 1)\n",
        "\n",
        "            mean = scatter(x, batch, dim=0, dim_size=batch_size,\n",
        "                           reduce='add').sum(dim=-1, keepdim=True) / norm\n",
        "\n",
        "            x = x - mean[batch]\n",
        "\n",
        "            var = scatter(x * x, batch, dim=0, dim_size=batch_size,\n",
        "                          reduce='add').sum(dim=-1, keepdim=True)\n",
        "            var = var / norm\n",
        "\n",
        "            out = x / (var.sqrt()[batch] + self.eps)\n",
        "\n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            out = out * self.weight + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'\n",
        "    r\"\"\"Applies layer normalization over each individual example in a batch\n",
        "    of node features as described in the `\"Instance Normalization: The Missing\n",
        "    Ingredient for Fast Stylization\" <https://arxiv.org/abs/1607.08022>`_\n",
        "    paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{x}^{\\prime}_i = \\frac{\\mathbf{x} -\n",
        "        \\textrm{E}[\\mathbf{x}]}{\\sqrt{\\textrm{Var}[\\mathbf{x}] + \\epsilon}}\n",
        "        \\odot \\gamma + \\beta\n",
        "\n",
        "    The mean and standard-deviation are calculated across all nodes and all\n",
        "    node channels separately for each object in a mini-batch.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        eps (float, optional): A value added to the denominator for numerical\n",
        "            stability. (default: :obj:`1e-5`)\n",
        "        affine (bool, optional): If set to :obj:`True`, this module has\n",
        "            learnable affine parameters :math:`\\gamma` and :math:`\\beta`.\n",
        "            (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, eps=1e-5, affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.eps = eps\n",
        "\n",
        "        if affine:\n",
        "            self.weight = Parameter(torch.Tensor([in_channels]))\n",
        "            self.bias = Parameter(torch.Tensor([in_channels]))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        ones(self.weight)\n",
        "        zeros(self.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, batch: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if batch is None:\n",
        "            x = x - x.mean()\n",
        "            out = x / (x.std(unbiased=False) + self.eps)\n",
        "\n",
        "        else:\n",
        "            batch_size = int(batch.max()) + 1\n",
        "\n",
        "            norm = degree(batch, batch_size, dtype=x.dtype).clamp_(min=1)\n",
        "            norm = norm.mul_(x.size(-1)).view(-1, 1)\n",
        "\n",
        "            mean = scatter(x, batch, dim=0, dim_size=batch_size,\n",
        "                           reduce='add').sum(dim=-1, keepdim=True) / norm\n",
        "\n",
        "            x = x - mean[batch]\n",
        "\n",
        "            var = scatter(x * x, batch, dim=0, dim_size=batch_size,\n",
        "                          reduce='add').sum(dim=-1, keepdim=True)\n",
        "            var = var / norm\n",
        "\n",
        "            out = x / (var.sqrt()[batch] + self.eps)\n",
        "\n",
        "        if self.weight is not None and self.bias is not None:\n",
        "            out = out * self.weight + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels})'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ_jr2mT5HfB"
      },
      "source": [
        "### 4.3 Main Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7FeUQuA1bvN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Sequential, Linear, ReLU, Bilinear\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch_geometric\n",
        "from torch_scatter import scatter_mean, scatter_add\n",
        "\n",
        "\n",
        "def get_gt_scene_graph_encoding_layer(num_node_features, num_edge_features):\n",
        "\n",
        "    class EdgeModel(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super(EdgeModel, self).__init__()\n",
        "            self.edge_mlp = Sequential(\n",
        "                Linear(2 * num_node_features + num_edge_features, num_edge_features),\n",
        "                ReLU(),\n",
        "                Linear(num_edge_features, num_edge_features)\n",
        "            )\n",
        "\n",
        "        def forward(self, src, dest, edge_attr, u, batch):\n",
        "            out = torch.cat([src, dest, edge_attr], dim=1)\n",
        "            return self.edge_mlp(out)\n",
        "\n",
        "    class NodeModel(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super(NodeModel, self).__init__()\n",
        "            self.node_mlp_1 = Sequential(\n",
        "                Linear(num_node_features + num_edge_features, num_node_features),\n",
        "                ReLU(),\n",
        "                Linear(num_node_features, num_node_features)\n",
        "            )\n",
        "            self.node_mlp_2 = Sequential(\n",
        "                Linear(2 * num_node_features, num_node_features),\n",
        "                ReLU(),\n",
        "                Linear(num_node_features, num_node_features)\n",
        "            )\n",
        "\n",
        "        def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "            row, col = edge_index\n",
        "            out = torch.cat([x[row], edge_attr], dim=1)\n",
        "            out = self.node_mlp_1(out)\n",
        "            out = scatter_mean(out, col, dim=0, dim_size=x.size(0))\n",
        "            out = torch.cat([x, out], dim=1)\n",
        "            return self.node_mlp_2(out)\n",
        "\n",
        "    op = torch_geometric.nn.MetaLayer(EdgeModel(), NodeModel())\n",
        "    return op\n",
        "\n",
        "\n",
        "class HierarchicalMatch(nn.Module):\n",
        "    def __init__(self, N=3, dim=512):\n",
        "        super(HierarchicalMatch, self).__init__()\n",
        "        self.N = N\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, joint, f, B):\n",
        "        N = self.N\n",
        "        dim = self.dim\n",
        "        b = torch.zeros(N, N, B).to('cuda')\n",
        "        for i, f_i in enumerate(f):\n",
        "            for j, f_ij in enumerate(f_i):\n",
        "                f_ij_tmp = torch.mean(f_ij, dim=1) # [B, 512]\n",
        "                b[i][j] = torch.bmm(joint.unsqueeze(1), f_ij_tmp.unsqueeze(-1)).squeeze() / torch.stack([torch.bmm(joint.unsqueeze(1), f[i][r].mean(dim=1).unsqueeze(-1)).squeeze() for r in range(N)]).sum(dim=0) # [B]\n",
        "\n",
        "        f_ii = []\n",
        "        for i, f_i in enumerate(f):\n",
        "            f_ii.append(torch.stack([b[i][j][:, None, None] * f_ij for j, f_ij in enumerate(f_i)]).sum(dim=0))\n",
        "\n",
        "        lambda_i = torch.zeros(N, B).to('cuda')\n",
        "        for i, f_i in enumerate(f_ii):\n",
        "            f_i_tmp = torch.mean(f_i, dim=1) # [B, 512]\n",
        "            lambda_i[i] = torch.bmm(joint.unsqueeze(1), f_i_tmp.unsqueeze(-1)).squeeze() / torch.stack([torch.bmm(joint.unsqueeze(1), f_ii[r].mean(dim=1).unsqueeze(-1)).squeeze() for r in range(N)]).sum(dim=0)\n",
        "\n",
        "        return torch.stack([lambda_i[i][:, None] * f_i.mean(dim=1) for i, f_i in enumerate(f_ii)]).sum(dim=0) # [B, 512]\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim=512):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.w = Linear(dim, dim)\n",
        "        nn.init.xavier_uniform_(self.w.weight)\n",
        "\n",
        "    def forward(self, audio_conv_list, video_conv_list):\n",
        "        (B, T, C) = audio_conv_list[0].shape\n",
        "        f_v = [[], [], []]\n",
        "        f_a = [[], [], []]\n",
        "        for i, video_conv in enumerate(video_conv_list):\n",
        "            for j, audio_conv in enumerate(audio_conv_list):\n",
        "                a_ij = F.softmax(torch.bmm(self.w(video_conv), audio_conv.permute(0, 2, 1)) / torch.sqrt(torch.tensor(video_conv.shape[-1]))) # [4, 10, 10]\n",
        "                f_v[i].append(torch.bmm(a_ij, audio_conv))\n",
        "                f_a[j].append(torch.bmm(a_ij.permute(0, 2, 1), video_conv))\n",
        "\n",
        "        return f_v, f_a\n",
        "\n",
        "\n",
        "class MgA(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(MgA, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, 1)\n",
        "        self.conv2 = nn.Conv1d(in_channels, out_channels, 3)\n",
        "        self.conv3 = nn.Conv1d(in_channels, out_channels, 5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv_1 = self.conv1(input).permute(0, 2, 1) # [B, 10, 512]\n",
        "        conv_2 = self.conv2(input).permute(0, 2, 1) # [B, 8, 512]\n",
        "        conv_3 = self.conv3(input).permute(0, 2, 1) # [B, 6, 512]\n",
        "\n",
        "        return conv_1, conv_2, conv_3\n",
        "\n",
        "\n",
        "class AVQA_GNN(nn.Module):\n",
        "\n",
        "    def __init__(self, args, num_node_features=512, num_edge_features=512):\n",
        "        super(AVQA_GNN, self).__init__()\n",
        "        self.scene_graph_encoding_layer = get_gt_scene_graph_encoding_layer(num_node_features=num_node_features, num_edge_features=num_edge_features)\n",
        "        self.query_graph_encoding_layer = get_gt_scene_graph_encoding_layer(num_node_features=num_node_features, num_edge_features=num_edge_features)\n",
        "        self.scene_graph_layernorm = LayerNorm(num_node_features)\n",
        "        self.query_graph_layernorm = LayerNorm(num_edge_features)\n",
        "        out_channels = 512\n",
        "        self.video_gat = GNNStack(in_channels=num_node_features,\n",
        "                                  out_channels=out_channels,\n",
        "                                  edge_attr_dim=num_edge_features,\n",
        "                                  num_layers=5,\n",
        "                                  heads=4,\n",
        "                                  dropout=0.1,\n",
        "                                  negative_slope=0.2)\n",
        "\n",
        "        self.query_gat = GNNStack(in_channels=num_node_features,\n",
        "                                  out_channels=out_channels,\n",
        "                                  edge_attr_dim=num_edge_features,\n",
        "                                  num_layers=5,\n",
        "                                  heads=4,\n",
        "                                  dropout=0.1,\n",
        "                                  negative_slope=0.2)\n",
        "        self.lin_v = Sequential(\n",
        "                Linear(out_channels, out_channels),\n",
        "                ReLU()\n",
        "            )\n",
        "        self.lin_q = Sequential(\n",
        "                Linear(out_channels, out_channels),\n",
        "                ReLU()\n",
        "            )\n",
        "        self.wv = Linear(out_channels, out_channels)\n",
        "        self.wq = Parameter(torch.zeros(1, 10))\n",
        "\n",
        "        self.joint_linear = Bilinear(out_channels, out_channels, out_channels)\n",
        "        self.lin_a = Sequential(\n",
        "                Linear(128, 512),\n",
        "                ReLU(),\n",
        "                Linear(512, 512)\n",
        "            )\n",
        "\n",
        "        self.mga_v = MgA(out_channels, out_channels)\n",
        "        self.mga_a = MgA(out_channels, out_channels)\n",
        "        self.cross_attn = CrossAttention(out_channels)\n",
        "        self.match_v = HierarchicalMatch(N=3, dim=out_channels)\n",
        "        self.match_a = HierarchicalMatch(N=3, dim=out_channels)\n",
        "        self.tanh_avq = nn.Tanh()\n",
        "        self.fc_answer_pred = nn.Linear(512, 42)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_v[0].weight)\n",
        "        nn.init.xavier_uniform_(self.lin_q[0].weight)\n",
        "        nn.init.xavier_uniform_(self.wv.weight)\n",
        "        nn.init.xavier_uniform_(self.wq)\n",
        "        nn.init.xavier_uniform_(self.joint_linear.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_a[0].weight)\n",
        "        nn.init.xavier_uniform_(self.lin_a[-1].weight)\n",
        "        nn.init.xavier_uniform_(self.fc_answer_pred.weight)\n",
        "\n",
        "    def forward(self, audio_feat, visual_feat, question_feat, sg_data, qg_data):\n",
        "        B = len(qg_data)\n",
        "\n",
        "        s_x_encoded_list, s_edge_attr_encoded_list = [], []\n",
        "        for i in range(len(sg_data)):\n",
        "            s_x_encoded, s_edge_attr_encoded, _ = self.scene_graph_encoding_layer(\n",
        "                x=sg_data[i].x,\n",
        "                edge_index=sg_data[i].edge_index,\n",
        "                edge_attr=sg_data[i].edge_attr,\n",
        "                u=None,\n",
        "                batch=sg_data[i].batch\n",
        "            )\n",
        "            s_x_encoded = self.scene_graph_layernorm(s_x_encoded, sg_data[i].batch)\n",
        "            s_x_encoded_list.append(s_x_encoded)\n",
        "            s_edge_attr_encoded_list.append(s_edge_attr_encoded)\n",
        "\n",
        "        q_x_encoded, q_edge_attr_encoded, _ = self.query_graph_encoding_layer(\n",
        "            x=qg_data.x,\n",
        "            edge_index=qg_data.edge_index,\n",
        "            edge_attr=qg_data.edge_attr,\n",
        "            u=None,\n",
        "            batch=qg_data.batch\n",
        "        )\n",
        "        q_x_encoded = self.query_graph_layernorm(q_x_encoded, qg_data.batch)\n",
        "\n",
        "        x_executed_list = []\n",
        "        for i, (s_x_encoded, s_edge_attr_encoded) in enumerate(zip(s_x_encoded_list, s_edge_attr_encoded_list)):\n",
        "            x_executed = self.video_gat(x=s_x_encoded, edge_index=sg_data[i].edge_index, edge_attr=s_edge_attr_encoded, batch=sg_data[i].batch)\n",
        "            x_executed_batch_list = []\n",
        "            for j in range(B):\n",
        "                x_executed_batch_list.append(x_executed[sg_data[i].batch==j].sum(dim=0))\n",
        "            x_executed = torch.stack(x_executed_batch_list) # [B, 512]\n",
        "            x_executed_list.append(x_executed)\n",
        "        x_executed_list = torch.stack(x_executed_list).permute(1, 0, 2) # [B, 10, 512]\n",
        "\n",
        "        q_executed = self.query_gat(x=q_x_encoded, edge_index=qg_data.edge_index, edge_attr=q_edge_attr_encoded, batch=qg_data.batch)\n",
        "        q_executed_batch_list = []\n",
        "        for i in range(B):\n",
        "            q_executed_batch_list.append(q_executed[qg_data.batch==i])\n",
        "        max_q = max(q.shape[0] for q in q_executed_batch_list)\n",
        "        for i, q in enumerate(q_executed_batch_list):\n",
        "            pad_size = max_q - q.shape[0]\n",
        "            if pad_size > 0:\n",
        "                q_executed_batch_list[i] = F.pad(q, (0, 0, 0, pad_size))\n",
        "        q_executed = torch.stack(q_executed_batch_list) # [B, q, 512]\n",
        "\n",
        "        video = self.lin_v(x_executed_list)\n",
        "        query = self.lin_q(q_executed)\n",
        "\n",
        "        sim = torch.bmm(video, query.permute(0, 2, 1)) # [B, n:10, q]\n",
        "\n",
        "        temporature = 1.0\n",
        "        v_joint = torch.bmm((sim/temporature).permute(0, 2, 1), video) # [B, q, 512]\n",
        "        v_joint = self.wv(v_joint).mean(dim=1) # [B, 512]\n",
        "\n",
        "        q_joint = torch.bmm((sim/temporature), query) # [B, 10, 512]\n",
        "        q_joint = (self.wq.unsqueeze(-1) * q_joint).sum(dim=1) # [B, 512]\n",
        "\n",
        "        vq_joint = self.joint_linear(v_joint, q_joint) # [B, 512]\n",
        "\n",
        "        audio_feat = self.lin_a(audio_feat)\n",
        "        (B, T, C) = audio_feat.shape\n",
        "\n",
        "        audio_conv_1, audio_conv_2, audio_conv_3 = self.mga_a(audio_feat.permute(0, 2, 1)) # [B, l_a, 512]\n",
        "        video_conv_1, video_conv_2, video_conv_3 = self.mga_v(visual_feat.permute(0, 2, 1)) # [B, l_v, 512]\n",
        "\n",
        "        audio_conv_list = [audio_conv_1, audio_conv_2, audio_conv_3]\n",
        "        video_conv_list = [video_conv_1, video_conv_2, video_conv_3]\n",
        "\n",
        "        f_v, f_a = self.cross_attn(audio_conv_list, video_conv_list) # [3, 3, B, 10, 512]\n",
        "\n",
        "        f_v = self.match_v(vq_joint, f_v, B) # [B, 512]\n",
        "        f_a = self.match_a(vq_joint, f_a, B)\n",
        "\n",
        "        z_v = F.sigmoid(vq_joint * f_v) # [B, 512]\n",
        "        z_a = F.sigmoid(vq_joint * f_a)\n",
        "\n",
        "        f_m = z_v * f_v + z_a * f_a # [B, 512]\n",
        "\n",
        "        avq_feat = f_m * question_feat.squeeze(1) # [B, 512]\n",
        "        avq_feat = self.tanh_avq(avq_feat)\n",
        "\n",
        "        answer_pred = self.fc_answer_pred(avq_feat)\n",
        "\n",
        "        return answer_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPT4mKXt73B3"
      },
      "source": [
        "## 5. Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B76wavF47e-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch_geometric\n",
        "\n",
        "\n",
        "def TransformImage(img):\n",
        "\n",
        "    transform_list = []\n",
        "    mean = [0.43216, 0.394666, 0.37645]\n",
        "    std = [0.22803, 0.22145, 0.216989]\n",
        "\n",
        "    transform_list.append(transforms.Resize([256,256]))\n",
        "    transform_list.append(transforms.ToTensor())\n",
        "    transform_list.append(transforms.Normalize(mean, std))\n",
        "    trans = transforms.Compose(transform_list)\n",
        "    frame_tensor = trans(img)\n",
        "\n",
        "    return frame_tensor\n",
        "\n",
        "def TransformImage_Resize(img):\n",
        "\n",
        "    transform_list = []\n",
        "    mean = [0.43216, 0.394666, 0.37645]\n",
        "    std = [0.22803, 0.22145, 0.216989]\n",
        "\n",
        "    transform_list.append(transforms.Resize([256,256]))\n",
        "    # transform_list.append(transforms.ToTensor())\n",
        "    # transform_list.append(transforms.Normalize(mean, std))\n",
        "    trans = transforms.Compose(transform_list)\n",
        "    frame_org = trans(img)\n",
        "\n",
        "    return frame_org\n",
        "\n",
        "\n",
        "def load_frame_info(img_path):\n",
        "\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # img2 = TransformImage_Resize(img)   # visualization\n",
        "    frame_tensor = TransformImage(img)\n",
        "\n",
        "    # return img2, frame_tensor    # visualization\n",
        "    return frame_tensor\n",
        "\n",
        "\n",
        "def image_info(frame_path):\n",
        "\n",
        "    img_list = os.listdir(frame_path)\n",
        "    img_list.sort()\n",
        "\n",
        "    select_img = []\n",
        "\n",
        "    for frame_idx in range(len(img_list)):\n",
        "        if frame_idx < 60:\n",
        "            video_frames_path = os.path.join(frame_path, str(frame_idx+1).zfill(6)+\".jpg\")\n",
        "            frame_tensor_info = load_frame_info(video_frames_path)\n",
        "            select_img.append(frame_tensor_info.cpu().numpy())\n",
        "\n",
        "\n",
        "    select_img = np.array(select_img)\n",
        "\n",
        "    # return org_img, select_img\n",
        "    return select_img\n",
        "\n",
        "\n",
        "\n",
        "def ids_to_multinomial(id, categories):\n",
        "    \"\"\" label encoding\n",
        "    Returns:\n",
        "      1d array, multimonial representation, e.g. [1,0,1,0,0,...]\n",
        "    \"\"\"\n",
        "    id_to_idx = {id: index for index, id in enumerate(categories)}\n",
        "\n",
        "    return id_to_idx[id]\n",
        "\n",
        "\n",
        "class AVQA_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, args, label, audios_feat_dir,\n",
        "                       clip_vit_b32_dir, clip_qst_dir):\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        samples = json.load(open(args['label_train'], 'r'))\n",
        "\n",
        "        # Question\n",
        "        ques_vocab = ['<pad>']\n",
        "        ans_vocab = []\n",
        "        i = 0\n",
        "        for sample in samples:\n",
        "            i += 1\n",
        "            question = sample['question_content'].rstrip().split(' ')\n",
        "            question[-1] = question[-1][:-1]\n",
        "\n",
        "            p = 0\n",
        "            for pos in range(len(question)):\n",
        "                if '<' in question[pos]:\n",
        "                    question[pos] = ast.literal_eval(sample['templ_values'])[p]\n",
        "                    p += 1\n",
        "            for wd in question:\n",
        "                if wd not in ques_vocab:\n",
        "                    ques_vocab.append(wd)\n",
        "            if sample['anser'] not in ans_vocab:\n",
        "                ans_vocab.append(sample['anser'])\n",
        "        # ques_vocab.append('fifth')\n",
        "\n",
        "        self.ques_vocab = ques_vocab\n",
        "        self.ans_vocab = ans_vocab\n",
        "        self.word_to_ix = {word: i for i, word in enumerate(self.ques_vocab)}\n",
        "\n",
        "        self.samples = json.load(open(label, 'r'))\n",
        "        self.max_len = 14    # question length\n",
        "\n",
        "        self.audios_feat_dir = audios_feat_dir\n",
        "\n",
        "        self.clip_vit_b32_dir = clip_vit_b32_dir\n",
        "        self.clip_qst_dir = clip_qst_dir\n",
        "        self.scene_graph_dir = args['scene_graph_dir']\n",
        "        self.query_graph_dir = args['query_graph_dir']\n",
        "        self.scene_graphs = np.load(os.path.join(self.scene_graph_dir, \"scene_graphs.npy\"), allow_pickle=True).item()\n",
        "        self.query_graphs = np.load(os.path.join(self.query_graph_dir, \"query_graphs.npy\"), allow_pickle=True).item()\n",
        "        self.scene_name2idx = json.load(open(os.path.join(self.scene_graph_dir, \"name2idx.json\"), 'r'))\n",
        "        self.query_name2idx = json.load(open(os.path.join(self.query_graph_dir, \"name2idx.json\"), 'r'))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def get_lstm_embeddings(self, question_input, sample):\n",
        "\n",
        "        question = sample['question_content'].rstrip().split(' ')\n",
        "        question[-1] = question[-1][:-1]\n",
        "\n",
        "        p = 0\n",
        "        for pos in range(len(question)):\n",
        "            if '<' in question[pos]:\n",
        "                question[pos] = ast.literal_eval(sample['templ_values'])[p]\n",
        "                p += 1\n",
        "        if len(question) < self.max_len:\n",
        "            n = self.max_len - len(question)\n",
        "            for i in range(n):\n",
        "                question.append('<pad>')\n",
        "\n",
        "        idxs = [self.word_to_ix[w] for w in question]\n",
        "        ques = torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "        return ques\n",
        "\n",
        "    def get_frames_spatial(self, video_name):\n",
        "\n",
        "        frames_path = os.path.join(self.frames_dir, video_name)\n",
        "        frames_spatial = image_info(frames_path)    # [T, 3, 224, 224]\n",
        "\n",
        "        return frames_spatial\n",
        "\n",
        "    def convert_to_pyg_graph(self, entities, relations):\n",
        "        x = torch.zeros(len(entities), 512)\n",
        "        edge_features = torch.zeros(len(relations), 512)\n",
        "        edge_topology = torch.zeros(len(relations), 2).long()\n",
        "\n",
        "        for i, entity in enumerate(entities):\n",
        "            x_idx = entity['span_embedding']\n",
        "            x[i] = x_idx\n",
        "\n",
        "        for i, relation in enumerate(relations):\n",
        "            edge_feature_idx = relation['relation_embedding']\n",
        "            edge_features[i] = edge_feature_idx\n",
        "            edge_topology[i] = torch.tensor((relation['subject'], relation['object']))\n",
        "\n",
        "        data = torch_geometric.data.Data(x=x, edge_index=edge_topology.t().contiguous(), edge_attr=edge_features)\n",
        "        return data\n",
        "\n",
        "    def convert_to_pyg_graphs(self, sg_this):\n",
        "        pyg_datas = []\n",
        "        for i in range(10):\n",
        "            sg = sg_this[i]\n",
        "            pyg_data = self.convert_to_pyg_graph(sg['entities'], sg['relations'])\n",
        "            pyg_datas.append(pyg_data)\n",
        "\n",
        "        return pyg_datas\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sample = self.samples[idx]\n",
        "        name = sample['video_id']\n",
        "        question_id = sample['question_id']\n",
        "\n",
        "        audio_feat = np.load(os.path.join(self.audios_feat_dir, name + '.npy'))\n",
        "        audio_feat = audio_feat[::6, :]\n",
        "\n",
        "        question_feat = np.load(os.path.join(self.clip_qst_dir, str(question_id) + '.npy'))\n",
        "\n",
        "        visual_CLIP_feat = np.load(os.path.join(self.clip_vit_b32_dir, name + '.npy'))\n",
        "        visual_feat = visual_CLIP_feat[::6, 0, :]\n",
        "\n",
        "        # visual_CLIP_feat = np.load(os.path.join(self.clip_vit_b32_dir, name + '.npy'))\n",
        "        # patch_feat = visual_CLIP_feat[:60, 1:, :]\n",
        "\n",
        "        #########################################################################\n",
        "        # one json for all scene_graph\n",
        "        id = self.scene_name2idx[name]\n",
        "        scene_graphs = self.scene_graphs[id]\n",
        "        sg_data = self.convert_to_pyg_graphs(scene_graphs) # list[pyg_data]\n",
        "\n",
        "        id = self.query_name2idx[str(question_id)]\n",
        "        query_graph = self.query_graphs[id]\n",
        "        qg_data = self.convert_to_pyg_graph(query_graph['entities'], query_graph['relations'])\n",
        "        #########################################################################\n",
        "\n",
        "        ### answer\n",
        "        answer = sample['anser']\n",
        "        answer_label = ids_to_multinomial(answer, self.ans_vocab)\n",
        "        answer_label = torch.from_numpy(np.array(answer_label)).long()\n",
        "\n",
        "        return (name, torch.from_numpy(audio_feat), torch.from_numpy(visual_feat), torch.from_numpy(question_feat), answer_label, sg_data, qg_data, question_id)\n",
        "\n",
        "\n",
        "def AVQA_dataset_collate_fn(data):\n",
        "\n",
        "    name, audio_feat, visual_feat, question_feat, answer_label, sg_data, qg_data, question_id = zip(*data)\n",
        "\n",
        "    audio_feat = torch.stack(audio_feat)\n",
        "    visual_feat = torch.stack(visual_feat)\n",
        "    question_feat = torch.stack(question_feat)\n",
        "    answer_label = torch.stack(answer_label)\n",
        "\n",
        "    sg_data_list = {i:[] for i in range(len(sg_data[0]))}\n",
        "    for sg_this in sg_data:\n",
        "        for i in range(len(sg_this)):\n",
        "            sg_data_list[i].append(sg_this[i])\n",
        "\n",
        "    sg_data_out = []\n",
        "    for i in range(len(sg_data[0])):\n",
        "        sg_data_out.append(torch_geometric.data.Batch.from_data_list(sg_data_list[i]))\n",
        "\n",
        "    qg_data = torch_geometric.data.Batch.from_data_list(qg_data)\n",
        "\n",
        "    return (name, audio_feat, visual_feat, question_feat, answer_label, sg_data_out, qg_data, question_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLFlKqwh7-m8"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_gCx64Q8DYD"
      },
      "source": [
        "### 6.1 Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzFMzlF376p1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "args = {}\n",
        "root_path = '/content/drive/MyDrive/AVQA-GNN/data/MUSIC-AVQA'\n",
        "dataset_path = '/content/drive/MyDrive/AVQA-GNN/dataset/split_que_id'\n",
        "\n",
        "### ======================== Dataset Configs ==========================\n",
        "args[\"audios_feat_dir\"] = os.path.join(root_path, 'vggish')\n",
        "args[\"clip_vit_b32_dir\"] = os.path.join(root_path, 'clip_vit_b32')\n",
        "args[\"clip_qst_dir\"] = os.path.join(root_path, 'clip_qst')\n",
        "args[\"clip_word_dir\"] = os.path.join(root_path, 'clip_word')\n",
        "args[\"frames_dir\"] = os.path.join(root_path, 'frames')\n",
        "args[\"scene_graph_dir\"] = os.path.join(root_path, 'scene_graphs_npy')\n",
        "args[\"query_graph_dir\"] = os.path.join(root_path, 'query_graphs_npy')\n",
        "\n",
        "### ======================== Label Configs ==========================\n",
        "args[\"label_train\"] = os.path.join(dataset_path, \"music_avqa_subset_train.json\")\n",
        "args[\"label_val\"] = os.path.join(dataset_path, \"music_avqa_subset_val.json\")\n",
        "args[\"label_test\"] = os.path.join(dataset_path, \"music_avqa_subset_test.json\")\n",
        "\n",
        "### ======================== Learning Configs ==========================\n",
        "args['batch_size'] =4\n",
        "args['epochs'] = 30\n",
        "args['lr'] = 1.2e-4\n",
        "args['seed'] = 1\n",
        "\n",
        "### ======================== Save Configs ==========================\n",
        "args[\"checkpoint\"] = 'AVQA_GNN_Net'\n",
        "args[\"model_save_dir\"] = '/content/drive/MyDrive/AVQA-GNN/models_avqa_gnn'\n",
        "args[\"mode\"] = 'train'\n",
        "\n",
        "### ======================== Runtime Configs ==========================\n",
        "args['log_interval'] = 5\n",
        "args['num_workers'] = 2\n",
        "args['gpu'] ='0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6tGMLATT5f9"
      },
      "source": [
        "### 6.2 Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siMcpYbKCO84",
        "outputId": "bcc6c72f-ffea-46a8-c3b8-7f039d1bff8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------- AVQA-GNN Training --------------- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datetime import datetime\n",
        "TIMESTAMP = \"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now())\n",
        "\n",
        "\n",
        "print(\"\\n--------------- AVQA-GNN Training --------------- \\n\")\n",
        "\n",
        "\n",
        "def train(args, model, train_loader, optimizer, criterion, epoch):\n",
        "\n",
        "    model.train()\n",
        "    print(\"-------- Training ... --------\")\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        name, audio_feat, visual_feat, question_feat, target, sg_data, qg_data, _ = sample\n",
        "        qg_data = qg_data.to('cuda')\n",
        "        for i in range(len(sg_data)):\n",
        "            sg_data[i] = sg_data[i].to('cuda')\n",
        "        audio_feat = audio_feat.to('cuda')\n",
        "        visual_feat = visual_feat.to('cuda')\n",
        "        question_feat = question_feat.to('cuda')\n",
        "        target = target.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output_qa = model(audio_feat, visual_feat, question_feat, sg_data, qg_data)\n",
        "        loss = criterion(output_qa, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(audio_feat), len(train_loader.dataset),\n",
        "                  100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def eval(model, val_loader, epoch):\n",
        "\n",
        "    model.eval()\n",
        "    total_qa = 0\n",
        "    correct_qa = 0\n",
        "    print(\"-------- Validating ... --------\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, sample in enumerate(val_loader):\n",
        "            name, audio_feat, visual_feat, question_feat, target, sg_data, qg_data, _ = sample\n",
        "            if sg_data[0].x.shape == (0, 512):\n",
        "                print(batch_idx)\n",
        "            qg_data = qg_data.to('cuda')\n",
        "            for i in range(len(sg_data)):\n",
        "                sg_data[i] = sg_data[i].to('cuda')\n",
        "            audio_feat = audio_feat.to('cuda')\n",
        "            visual_feat = visual_feat.to('cuda')\n",
        "            question_feat = question_feat.to('cuda')\n",
        "            target = target.to('cuda')\n",
        "\n",
        "            preds_qa = model(audio_feat, visual_feat, question_feat, sg_data, qg_data)\n",
        "\n",
        "            _, predicted = torch.max(preds_qa.data, 1)\n",
        "            total_qa += preds_qa.size(0)\n",
        "            correct_qa += (predicted == target).sum().item()\n",
        "\n",
        "    print('Current Acc: %.2f %%' % (100 * correct_qa / total_qa))\n",
        "\n",
        "    return 100 * correct_qa / total_qa\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args['gpu']\n",
        "    torch.manual_seed(args['seed'])\n",
        "    os.makedirs(args['model_save_dir'], exist_ok=True)\n",
        "\n",
        "    tensorboard_name = args['checkpoint']\n",
        "\n",
        "    model = AVQA_GNN(args)\n",
        "    model = nn.DataParallel(model).to('cuda')\n",
        "\n",
        "    train_dataset = AVQA_dataset(label = args['label_train'],\n",
        "                                 args = args,\n",
        "                                 audios_feat_dir = args['audios_feat_dir'],\n",
        "                                 clip_vit_b32_dir = args['clip_vit_b32_dir'],\n",
        "                                 clip_qst_dir = args['clip_qst_dir'])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=args['num_workers'], collate_fn=AVQA_dataset_collate_fn, drop_last=True)\n",
        "\n",
        "    val_dataset = AVQA_dataset(label = args['label_val'],\n",
        "                               args = args,\n",
        "                               audios_feat_dir = args['audios_feat_dir'],\n",
        "                               clip_vit_b32_dir = args['clip_vit_b32_dir'],\n",
        "                               clip_qst_dir = args['clip_qst_dir'])\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args['batch_size'], shuffle=False, num_workers=args['num_workers'], collate_fn=AVQA_dataset_collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    best_acc = 0\n",
        "    best_epoch = 0\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "\n",
        "        # train for one epoch\n",
        "        train(args, model, train_loader, optimizer, criterion, epoch=epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        scheduler.step(epoch)\n",
        "        current_acc = eval(model, val_loader, epoch)\n",
        "        if current_acc >= best_acc:\n",
        "            best_acc = current_acc\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), os.path.join(args['model_save_dir'], args['checkpoint'] + \".pt\"))\n",
        "\n",
        "        print(\"Best Acc: %.2f %%\"%best_acc)\n",
        "        print(\"Best Epoch: \", best_epoch)\n",
        "        print(\"*\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVl0oHeCUbPQ",
        "outputId": "4e2156be-31e1-47c7-a3d2-caaf267736d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------- Training ... --------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-c3e10dd018ee>:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  a_ij = F.softmax(torch.bmm(self.w(video_conv), audio_conv.permute(0, 2, 1)) / torch.sqrt(torch.tensor(video_conv.shape[-1]))) # [4, 10, 10]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1093 (0%)]\tLoss: 3.662083\n",
            "Train Epoch: 1 [20/1093 (2%)]\tLoss: 3.617024\n",
            "Train Epoch: 1 [40/1093 (4%)]\tLoss: 3.587315\n",
            "Train Epoch: 1 [60/1093 (5%)]\tLoss: 3.707588\n",
            "Train Epoch: 1 [80/1093 (7%)]\tLoss: 3.680830\n",
            "Train Epoch: 1 [100/1093 (9%)]\tLoss: 3.552197\n",
            "Train Epoch: 1 [120/1093 (11%)]\tLoss: 3.661313\n",
            "Train Epoch: 1 [140/1093 (13%)]\tLoss: 3.673157\n",
            "Train Epoch: 1 [160/1093 (15%)]\tLoss: 3.620560\n",
            "Train Epoch: 1 [180/1093 (16%)]\tLoss: 3.544400\n",
            "Train Epoch: 1 [200/1093 (18%)]\tLoss: 3.560287\n",
            "Train Epoch: 1 [220/1093 (20%)]\tLoss: 3.139659\n",
            "Train Epoch: 1 [240/1093 (22%)]\tLoss: 3.499409\n",
            "Train Epoch: 1 [260/1093 (24%)]\tLoss: 3.131776\n",
            "Train Epoch: 1 [280/1093 (26%)]\tLoss: 3.022007\n",
            "Train Epoch: 1 [300/1093 (27%)]\tLoss: 2.819176\n",
            "Train Epoch: 1 [320/1093 (29%)]\tLoss: 3.010092\n",
            "Train Epoch: 1 [340/1093 (31%)]\tLoss: 3.729508\n",
            "Train Epoch: 1 [360/1093 (33%)]\tLoss: 3.325577\n",
            "Train Epoch: 1 [380/1093 (35%)]\tLoss: 2.987315\n",
            "Train Epoch: 1 [400/1093 (37%)]\tLoss: 3.035258\n",
            "Train Epoch: 1 [420/1093 (38%)]\tLoss: 2.248495\n",
            "Train Epoch: 1 [440/1093 (40%)]\tLoss: 3.199953\n",
            "Train Epoch: 1 [460/1093 (42%)]\tLoss: 3.055883\n",
            "Train Epoch: 1 [480/1093 (44%)]\tLoss: 3.485665\n",
            "Train Epoch: 1 [500/1093 (46%)]\tLoss: 2.337689\n",
            "Train Epoch: 1 [520/1093 (48%)]\tLoss: 2.177349\n",
            "Train Epoch: 1 [540/1093 (49%)]\tLoss: 1.710423\n",
            "Train Epoch: 1 [560/1093 (51%)]\tLoss: 3.802789\n",
            "Train Epoch: 1 [580/1093 (53%)]\tLoss: 2.453354\n",
            "Train Epoch: 1 [600/1093 (55%)]\tLoss: 3.295430\n",
            "Train Epoch: 1 [620/1093 (57%)]\tLoss: 4.154106\n",
            "Train Epoch: 1 [640/1093 (59%)]\tLoss: 4.460518\n",
            "Train Epoch: 1 [660/1093 (60%)]\tLoss: 2.911517\n",
            "Train Epoch: 1 [680/1093 (62%)]\tLoss: 2.867090\n",
            "Train Epoch: 1 [700/1093 (64%)]\tLoss: 2.770761\n",
            "Train Epoch: 1 [720/1093 (66%)]\tLoss: 4.224581\n",
            "Train Epoch: 1 [740/1093 (68%)]\tLoss: 3.760228\n",
            "Train Epoch: 1 [760/1093 (70%)]\tLoss: 2.946555\n",
            "Train Epoch: 1 [780/1093 (71%)]\tLoss: 3.594735\n",
            "Train Epoch: 1 [800/1093 (73%)]\tLoss: 3.557742\n",
            "Train Epoch: 1 [820/1093 (75%)]\tLoss: 2.842331\n",
            "Train Epoch: 1 [840/1093 (77%)]\tLoss: 4.182264\n",
            "Train Epoch: 1 [860/1093 (79%)]\tLoss: 2.957638\n",
            "Train Epoch: 1 [880/1093 (81%)]\tLoss: 3.822870\n",
            "Train Epoch: 1 [900/1093 (82%)]\tLoss: 3.285426\n",
            "Train Epoch: 1 [920/1093 (84%)]\tLoss: 3.697433\n",
            "Train Epoch: 1 [940/1093 (86%)]\tLoss: 3.032509\n",
            "Train Epoch: 1 [960/1093 (88%)]\tLoss: 3.200390\n",
            "Train Epoch: 1 [980/1093 (90%)]\tLoss: 3.268229\n",
            "Train Epoch: 1 [1000/1093 (92%)]\tLoss: 3.023107\n",
            "Train Epoch: 1 [1020/1093 (93%)]\tLoss: 3.445628\n",
            "Train Epoch: 1 [1040/1093 (95%)]\tLoss: 2.000727\n",
            "Train Epoch: 1 [1060/1093 (97%)]\tLoss: 3.202949\n",
            "Train Epoch: 1 [1080/1093 (99%)]\tLoss: 2.931255\n",
            "-------- Validating ... --------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Acc: 25.64 %\n",
            "Best Acc: 25.64 %\n",
            "Best Epoch:  1\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 2 [0/1093 (0%)]\tLoss: 3.570112\n",
            "Train Epoch: 2 [20/1093 (2%)]\tLoss: 1.713591\n",
            "Train Epoch: 2 [40/1093 (4%)]\tLoss: 2.795732\n",
            "Train Epoch: 2 [60/1093 (5%)]\tLoss: 2.546050\n",
            "Train Epoch: 2 [80/1093 (7%)]\tLoss: 2.476767\n",
            "Train Epoch: 2 [100/1093 (9%)]\tLoss: 3.117114\n",
            "Train Epoch: 2 [120/1093 (11%)]\tLoss: 3.130617\n",
            "Train Epoch: 2 [140/1093 (13%)]\tLoss: 4.203146\n",
            "Train Epoch: 2 [160/1093 (15%)]\tLoss: 2.634593\n",
            "Train Epoch: 2 [180/1093 (16%)]\tLoss: 3.339999\n",
            "Train Epoch: 2 [200/1093 (18%)]\tLoss: 2.801528\n",
            "Train Epoch: 2 [220/1093 (20%)]\tLoss: 2.875941\n",
            "Train Epoch: 2 [240/1093 (22%)]\tLoss: 2.714263\n",
            "Train Epoch: 2 [260/1093 (24%)]\tLoss: 3.368569\n",
            "Train Epoch: 2 [280/1093 (26%)]\tLoss: 2.001953\n",
            "Train Epoch: 2 [300/1093 (27%)]\tLoss: 3.598702\n",
            "Train Epoch: 2 [320/1093 (29%)]\tLoss: 3.188379\n",
            "Train Epoch: 2 [340/1093 (31%)]\tLoss: 2.139091\n",
            "Train Epoch: 2 [360/1093 (33%)]\tLoss: 2.467112\n",
            "Train Epoch: 2 [380/1093 (35%)]\tLoss: 2.362994\n",
            "Train Epoch: 2 [400/1093 (37%)]\tLoss: 4.534442\n",
            "Train Epoch: 2 [420/1093 (38%)]\tLoss: 3.874931\n",
            "Train Epoch: 2 [440/1093 (40%)]\tLoss: 3.780167\n",
            "Train Epoch: 2 [460/1093 (42%)]\tLoss: 4.253132\n",
            "Train Epoch: 2 [480/1093 (44%)]\tLoss: 3.807435\n",
            "Train Epoch: 2 [500/1093 (46%)]\tLoss: 4.238715\n",
            "Train Epoch: 2 [520/1093 (48%)]\tLoss: 4.247685\n",
            "Train Epoch: 2 [540/1093 (49%)]\tLoss: 4.011647\n",
            "Train Epoch: 2 [560/1093 (51%)]\tLoss: 4.444007\n",
            "Train Epoch: 2 [580/1093 (53%)]\tLoss: 4.095860\n",
            "Train Epoch: 2 [600/1093 (55%)]\tLoss: 3.832089\n",
            "Train Epoch: 2 [620/1093 (57%)]\tLoss: 3.491890\n",
            "Train Epoch: 2 [640/1093 (59%)]\tLoss: 3.474302\n",
            "Train Epoch: 2 [660/1093 (60%)]\tLoss: 2.843808\n",
            "Train Epoch: 2 [680/1093 (62%)]\tLoss: 3.746571\n",
            "Train Epoch: 2 [700/1093 (64%)]\tLoss: 4.143878\n",
            "Train Epoch: 2 [720/1093 (66%)]\tLoss: 3.533145\n",
            "Train Epoch: 2 [740/1093 (68%)]\tLoss: 3.034596\n",
            "Train Epoch: 2 [760/1093 (70%)]\tLoss: 3.166494\n",
            "Train Epoch: 2 [780/1093 (71%)]\tLoss: 3.398119\n",
            "Train Epoch: 2 [800/1093 (73%)]\tLoss: 3.730097\n",
            "Train Epoch: 2 [820/1093 (75%)]\tLoss: 3.156257\n",
            "Train Epoch: 2 [840/1093 (77%)]\tLoss: 3.782313\n",
            "Train Epoch: 2 [860/1093 (79%)]\tLoss: 2.943787\n",
            "Train Epoch: 2 [880/1093 (81%)]\tLoss: 3.427011\n",
            "Train Epoch: 2 [900/1093 (82%)]\tLoss: 3.573448\n",
            "Train Epoch: 2 [920/1093 (84%)]\tLoss: 3.491166\n",
            "Train Epoch: 2 [940/1093 (86%)]\tLoss: 2.722019\n",
            "Train Epoch: 2 [960/1093 (88%)]\tLoss: 3.096285\n",
            "Train Epoch: 2 [980/1093 (90%)]\tLoss: 2.847417\n",
            "Train Epoch: 2 [1000/1093 (92%)]\tLoss: 3.609599\n",
            "Train Epoch: 2 [1020/1093 (93%)]\tLoss: 2.781197\n",
            "Train Epoch: 2 [1040/1093 (95%)]\tLoss: 2.376704\n",
            "Train Epoch: 2 [1060/1093 (97%)]\tLoss: 4.489153\n",
            "Train Epoch: 2 [1080/1093 (99%)]\tLoss: 2.991894\n",
            "-------- Validating ... --------\n",
            "Current Acc: 35.90 %\n",
            "Best Acc: 35.90 %\n",
            "Best Epoch:  2\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 3 [0/1093 (0%)]\tLoss: 4.057815\n",
            "Train Epoch: 3 [20/1093 (2%)]\tLoss: 1.873985\n",
            "Train Epoch: 3 [40/1093 (4%)]\tLoss: 1.886922\n",
            "Train Epoch: 3 [60/1093 (5%)]\tLoss: 1.750075\n",
            "Train Epoch: 3 [80/1093 (7%)]\tLoss: 1.368225\n",
            "Train Epoch: 3 [100/1093 (9%)]\tLoss: 2.250052\n",
            "Train Epoch: 3 [120/1093 (11%)]\tLoss: 1.712054\n",
            "Train Epoch: 3 [140/1093 (13%)]\tLoss: 3.844701\n",
            "Train Epoch: 3 [160/1093 (15%)]\tLoss: 2.289529\n",
            "Train Epoch: 3 [180/1093 (16%)]\tLoss: 3.861997\n",
            "Train Epoch: 3 [200/1093 (18%)]\tLoss: 2.514479\n",
            "Train Epoch: 3 [220/1093 (20%)]\tLoss: 1.949320\n",
            "Train Epoch: 3 [240/1093 (22%)]\tLoss: 3.045587\n",
            "Train Epoch: 3 [260/1093 (24%)]\tLoss: 1.580205\n",
            "Train Epoch: 3 [280/1093 (26%)]\tLoss: 3.036436\n",
            "Train Epoch: 3 [300/1093 (27%)]\tLoss: 2.890616\n",
            "Train Epoch: 3 [320/1093 (29%)]\tLoss: 2.090132\n",
            "Train Epoch: 3 [340/1093 (31%)]\tLoss: 1.863973\n",
            "Train Epoch: 3 [360/1093 (33%)]\tLoss: 2.317985\n",
            "Train Epoch: 3 [380/1093 (35%)]\tLoss: 2.415922\n",
            "Train Epoch: 3 [400/1093 (37%)]\tLoss: 2.300242\n",
            "Train Epoch: 3 [420/1093 (38%)]\tLoss: 2.890901\n",
            "Train Epoch: 3 [440/1093 (40%)]\tLoss: 1.264372\n",
            "Train Epoch: 3 [460/1093 (42%)]\tLoss: 4.185352\n",
            "Train Epoch: 3 [480/1093 (44%)]\tLoss: 1.962768\n",
            "Train Epoch: 3 [500/1093 (46%)]\tLoss: 3.288611\n",
            "Train Epoch: 3 [520/1093 (48%)]\tLoss: 1.238821\n",
            "Train Epoch: 3 [540/1093 (49%)]\tLoss: 2.016488\n",
            "Train Epoch: 3 [560/1093 (51%)]\tLoss: 1.733081\n",
            "Train Epoch: 3 [580/1093 (53%)]\tLoss: 3.321267\n",
            "Train Epoch: 3 [600/1093 (55%)]\tLoss: 3.098205\n",
            "Train Epoch: 3 [620/1093 (57%)]\tLoss: 1.737763\n",
            "Train Epoch: 3 [640/1093 (59%)]\tLoss: 3.154929\n",
            "Train Epoch: 3 [660/1093 (60%)]\tLoss: 1.842785\n",
            "Train Epoch: 3 [680/1093 (62%)]\tLoss: 1.591234\n",
            "Train Epoch: 3 [700/1093 (64%)]\tLoss: 1.665743\n",
            "Train Epoch: 3 [720/1093 (66%)]\tLoss: 2.017755\n",
            "Train Epoch: 3 [740/1093 (68%)]\tLoss: 1.189586\n",
            "Train Epoch: 3 [760/1093 (70%)]\tLoss: 2.208707\n",
            "Train Epoch: 3 [780/1093 (71%)]\tLoss: 3.513194\n",
            "Train Epoch: 3 [800/1093 (73%)]\tLoss: 1.317688\n",
            "Train Epoch: 3 [820/1093 (75%)]\tLoss: 2.608323\n",
            "Train Epoch: 3 [840/1093 (77%)]\tLoss: 3.618083\n",
            "Train Epoch: 3 [860/1093 (79%)]\tLoss: 2.088298\n",
            "Train Epoch: 3 [880/1093 (81%)]\tLoss: 2.370532\n",
            "Train Epoch: 3 [900/1093 (82%)]\tLoss: 3.802846\n",
            "Train Epoch: 3 [920/1093 (84%)]\tLoss: 1.232290\n",
            "Train Epoch: 3 [940/1093 (86%)]\tLoss: 3.812151\n",
            "Train Epoch: 3 [960/1093 (88%)]\tLoss: 1.379111\n",
            "Train Epoch: 3 [980/1093 (90%)]\tLoss: 2.921688\n",
            "Train Epoch: 3 [1000/1093 (92%)]\tLoss: 2.461093\n",
            "Train Epoch: 3 [1020/1093 (93%)]\tLoss: 2.536981\n",
            "Train Epoch: 3 [1040/1093 (95%)]\tLoss: 2.829402\n",
            "Train Epoch: 3 [1060/1093 (97%)]\tLoss: 3.280344\n",
            "Train Epoch: 3 [1080/1093 (99%)]\tLoss: 3.641598\n",
            "-------- Validating ... --------\n",
            "Current Acc: 41.67 %\n",
            "Best Acc: 41.67 %\n",
            "Best Epoch:  3\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 4 [0/1093 (0%)]\tLoss: 2.977767\n",
            "Train Epoch: 4 [20/1093 (2%)]\tLoss: 3.569429\n",
            "Train Epoch: 4 [40/1093 (4%)]\tLoss: 1.061090\n",
            "Train Epoch: 4 [60/1093 (5%)]\tLoss: 1.727391\n",
            "Train Epoch: 4 [80/1093 (7%)]\tLoss: 1.924011\n",
            "Train Epoch: 4 [100/1093 (9%)]\tLoss: 2.109066\n",
            "Train Epoch: 4 [120/1093 (11%)]\tLoss: 1.892592\n",
            "Train Epoch: 4 [140/1093 (13%)]\tLoss: 2.575961\n",
            "Train Epoch: 4 [160/1093 (15%)]\tLoss: 1.802104\n",
            "Train Epoch: 4 [180/1093 (16%)]\tLoss: 1.326862\n",
            "Train Epoch: 4 [200/1093 (18%)]\tLoss: 0.540370\n",
            "Train Epoch: 4 [220/1093 (20%)]\tLoss: 2.725055\n",
            "Train Epoch: 4 [240/1093 (22%)]\tLoss: 3.479182\n",
            "Train Epoch: 4 [260/1093 (24%)]\tLoss: 2.004981\n",
            "Train Epoch: 4 [280/1093 (26%)]\tLoss: 1.992596\n",
            "Train Epoch: 4 [300/1093 (27%)]\tLoss: 1.806691\n",
            "Train Epoch: 4 [320/1093 (29%)]\tLoss: 1.013864\n",
            "Train Epoch: 4 [340/1093 (31%)]\tLoss: 0.922056\n",
            "Train Epoch: 4 [360/1093 (33%)]\tLoss: 1.892425\n",
            "Train Epoch: 4 [380/1093 (35%)]\tLoss: 1.185259\n",
            "Train Epoch: 4 [400/1093 (37%)]\tLoss: 2.294703\n",
            "Train Epoch: 4 [420/1093 (38%)]\tLoss: 1.616527\n",
            "Train Epoch: 4 [440/1093 (40%)]\tLoss: 2.235265\n",
            "Train Epoch: 4 [460/1093 (42%)]\tLoss: 3.166143\n",
            "Train Epoch: 4 [480/1093 (44%)]\tLoss: 3.375181\n",
            "Train Epoch: 4 [500/1093 (46%)]\tLoss: 2.993330\n",
            "Train Epoch: 4 [520/1093 (48%)]\tLoss: 1.749823\n",
            "Train Epoch: 4 [540/1093 (49%)]\tLoss: 1.994655\n",
            "Train Epoch: 4 [560/1093 (51%)]\tLoss: 1.273897\n",
            "Train Epoch: 4 [580/1093 (53%)]\tLoss: 2.230422\n",
            "Train Epoch: 4 [600/1093 (55%)]\tLoss: 1.653253\n",
            "Train Epoch: 4 [620/1093 (57%)]\tLoss: 1.653361\n",
            "Train Epoch: 4 [640/1093 (59%)]\tLoss: 0.845110\n",
            "Train Epoch: 4 [660/1093 (60%)]\tLoss: 2.221379\n",
            "Train Epoch: 4 [680/1093 (62%)]\tLoss: 2.289402\n",
            "Train Epoch: 4 [700/1093 (64%)]\tLoss: 1.272421\n",
            "Train Epoch: 4 [720/1093 (66%)]\tLoss: 0.573494\n",
            "Train Epoch: 4 [740/1093 (68%)]\tLoss: 0.715710\n",
            "Train Epoch: 4 [760/1093 (70%)]\tLoss: 1.689891\n",
            "Train Epoch: 4 [780/1093 (71%)]\tLoss: 0.890303\n",
            "Train Epoch: 4 [800/1093 (73%)]\tLoss: 1.809207\n",
            "Train Epoch: 4 [820/1093 (75%)]\tLoss: 1.309311\n",
            "Train Epoch: 4 [840/1093 (77%)]\tLoss: 3.002481\n",
            "Train Epoch: 4 [860/1093 (79%)]\tLoss: 1.790393\n",
            "Train Epoch: 4 [880/1093 (81%)]\tLoss: 1.475116\n",
            "Train Epoch: 4 [900/1093 (82%)]\tLoss: 1.225517\n",
            "Train Epoch: 4 [920/1093 (84%)]\tLoss: 3.283878\n",
            "Train Epoch: 4 [940/1093 (86%)]\tLoss: 1.009375\n",
            "Train Epoch: 4 [960/1093 (88%)]\tLoss: 1.493118\n",
            "Train Epoch: 4 [980/1093 (90%)]\tLoss: 2.058596\n",
            "Train Epoch: 4 [1000/1093 (92%)]\tLoss: 1.261583\n",
            "Train Epoch: 4 [1020/1093 (93%)]\tLoss: 1.587165\n",
            "Train Epoch: 4 [1040/1093 (95%)]\tLoss: 3.251064\n",
            "Train Epoch: 4 [1060/1093 (97%)]\tLoss: 2.410168\n",
            "Train Epoch: 4 [1080/1093 (99%)]\tLoss: 1.411984\n",
            "-------- Validating ... --------\n",
            "Current Acc: 42.31 %\n",
            "Best Acc: 42.31 %\n",
            "Best Epoch:  4\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 5 [0/1093 (0%)]\tLoss: 2.319298\n",
            "Train Epoch: 5 [20/1093 (2%)]\tLoss: 2.036220\n",
            "Train Epoch: 5 [40/1093 (4%)]\tLoss: 2.991389\n",
            "Train Epoch: 5 [60/1093 (5%)]\tLoss: 1.527102\n",
            "Train Epoch: 5 [80/1093 (7%)]\tLoss: 2.306302\n",
            "Train Epoch: 5 [100/1093 (9%)]\tLoss: 2.611134\n",
            "Train Epoch: 5 [120/1093 (11%)]\tLoss: 1.431111\n",
            "Train Epoch: 5 [140/1093 (13%)]\tLoss: 1.712934\n",
            "Train Epoch: 5 [160/1093 (15%)]\tLoss: 1.965153\n",
            "Train Epoch: 5 [180/1093 (16%)]\tLoss: 2.034893\n",
            "Train Epoch: 5 [200/1093 (18%)]\tLoss: 3.012358\n",
            "Train Epoch: 5 [220/1093 (20%)]\tLoss: 1.863268\n",
            "Train Epoch: 5 [240/1093 (22%)]\tLoss: 0.960914\n",
            "Train Epoch: 5 [260/1093 (24%)]\tLoss: 3.889623\n",
            "Train Epoch: 5 [280/1093 (26%)]\tLoss: 3.194397\n",
            "Train Epoch: 5 [300/1093 (27%)]\tLoss: 1.740817\n",
            "Train Epoch: 5 [320/1093 (29%)]\tLoss: 0.647405\n",
            "Train Epoch: 5 [340/1093 (31%)]\tLoss: 1.355291\n",
            "Train Epoch: 5 [360/1093 (33%)]\tLoss: 3.388924\n",
            "Train Epoch: 5 [380/1093 (35%)]\tLoss: 1.467108\n",
            "Train Epoch: 5 [400/1093 (37%)]\tLoss: 1.047549\n",
            "Train Epoch: 5 [420/1093 (38%)]\tLoss: 2.446552\n",
            "Train Epoch: 5 [440/1093 (40%)]\tLoss: 1.755310\n",
            "Train Epoch: 5 [460/1093 (42%)]\tLoss: 1.907617\n",
            "Train Epoch: 5 [480/1093 (44%)]\tLoss: 3.653648\n",
            "Train Epoch: 5 [500/1093 (46%)]\tLoss: 1.678679\n",
            "Train Epoch: 5 [520/1093 (48%)]\tLoss: 2.160967\n",
            "Train Epoch: 5 [540/1093 (49%)]\tLoss: 1.407632\n",
            "Train Epoch: 5 [560/1093 (51%)]\tLoss: 0.535789\n",
            "Train Epoch: 5 [580/1093 (53%)]\tLoss: 0.828365\n",
            "Train Epoch: 5 [600/1093 (55%)]\tLoss: 0.872744\n",
            "Train Epoch: 5 [620/1093 (57%)]\tLoss: 1.240447\n",
            "Train Epoch: 5 [640/1093 (59%)]\tLoss: 0.659372\n",
            "Train Epoch: 5 [660/1093 (60%)]\tLoss: 1.026860\n",
            "Train Epoch: 5 [680/1093 (62%)]\tLoss: 2.517889\n",
            "Train Epoch: 5 [700/1093 (64%)]\tLoss: 0.597762\n",
            "Train Epoch: 5 [720/1093 (66%)]\tLoss: 1.258143\n",
            "Train Epoch: 5 [740/1093 (68%)]\tLoss: 1.812011\n",
            "Train Epoch: 5 [760/1093 (70%)]\tLoss: 1.373138\n",
            "Train Epoch: 5 [780/1093 (71%)]\tLoss: 1.564686\n",
            "Train Epoch: 5 [800/1093 (73%)]\tLoss: 1.701305\n",
            "Train Epoch: 5 [820/1093 (75%)]\tLoss: 2.619544\n",
            "Train Epoch: 5 [840/1093 (77%)]\tLoss: 1.482908\n",
            "Train Epoch: 5 [860/1093 (79%)]\tLoss: 2.607118\n",
            "Train Epoch: 5 [880/1093 (81%)]\tLoss: 1.614523\n",
            "Train Epoch: 5 [900/1093 (82%)]\tLoss: 1.654665\n",
            "Train Epoch: 5 [920/1093 (84%)]\tLoss: 1.292055\n",
            "Train Epoch: 5 [940/1093 (86%)]\tLoss: 1.175407\n",
            "Train Epoch: 5 [960/1093 (88%)]\tLoss: 2.632914\n",
            "Train Epoch: 5 [980/1093 (90%)]\tLoss: 1.546310\n",
            "Train Epoch: 5 [1000/1093 (92%)]\tLoss: 1.136119\n",
            "Train Epoch: 5 [1020/1093 (93%)]\tLoss: 1.345698\n",
            "Train Epoch: 5 [1040/1093 (95%)]\tLoss: 2.714309\n",
            "Train Epoch: 5 [1060/1093 (97%)]\tLoss: 1.359972\n",
            "Train Epoch: 5 [1080/1093 (99%)]\tLoss: 2.749752\n",
            "-------- Validating ... --------\n",
            "Current Acc: 48.72 %\n",
            "Best Acc: 48.72 %\n",
            "Best Epoch:  5\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 6 [0/1093 (0%)]\tLoss: 1.160830\n",
            "Train Epoch: 6 [20/1093 (2%)]\tLoss: 2.087868\n",
            "Train Epoch: 6 [40/1093 (4%)]\tLoss: 1.392366\n",
            "Train Epoch: 6 [60/1093 (5%)]\tLoss: 1.078062\n",
            "Train Epoch: 6 [80/1093 (7%)]\tLoss: 1.733997\n",
            "Train Epoch: 6 [100/1093 (9%)]\tLoss: 1.411221\n",
            "Train Epoch: 6 [120/1093 (11%)]\tLoss: 1.237582\n",
            "Train Epoch: 6 [140/1093 (13%)]\tLoss: 1.904313\n",
            "Train Epoch: 6 [160/1093 (15%)]\tLoss: 2.429684\n",
            "Train Epoch: 6 [180/1093 (16%)]\tLoss: 1.651822\n",
            "Train Epoch: 6 [200/1093 (18%)]\tLoss: 0.723959\n",
            "Train Epoch: 6 [220/1093 (20%)]\tLoss: 1.172796\n",
            "Train Epoch: 6 [240/1093 (22%)]\tLoss: 2.205967\n",
            "Train Epoch: 6 [260/1093 (24%)]\tLoss: 2.986139\n",
            "Train Epoch: 6 [280/1093 (26%)]\tLoss: 0.948987\n",
            "Train Epoch: 6 [300/1093 (27%)]\tLoss: 1.596645\n",
            "Train Epoch: 6 [320/1093 (29%)]\tLoss: 2.703810\n",
            "Train Epoch: 6 [340/1093 (31%)]\tLoss: 1.736666\n",
            "Train Epoch: 6 [360/1093 (33%)]\tLoss: 1.676269\n",
            "Train Epoch: 6 [380/1093 (35%)]\tLoss: 2.402291\n",
            "Train Epoch: 6 [400/1093 (37%)]\tLoss: 1.624193\n",
            "Train Epoch: 6 [420/1093 (38%)]\tLoss: 2.319368\n",
            "Train Epoch: 6 [440/1093 (40%)]\tLoss: 1.759856\n",
            "Train Epoch: 6 [460/1093 (42%)]\tLoss: 2.113774\n",
            "Train Epoch: 6 [480/1093 (44%)]\tLoss: 0.695870\n",
            "Train Epoch: 6 [500/1093 (46%)]\tLoss: 2.410120\n",
            "Train Epoch: 6 [520/1093 (48%)]\tLoss: 0.573331\n",
            "Train Epoch: 6 [540/1093 (49%)]\tLoss: 1.156951\n",
            "Train Epoch: 6 [560/1093 (51%)]\tLoss: 1.465054\n",
            "Train Epoch: 6 [580/1093 (53%)]\tLoss: 0.849458\n",
            "Train Epoch: 6 [600/1093 (55%)]\tLoss: 0.667826\n",
            "Train Epoch: 6 [620/1093 (57%)]\tLoss: 1.682738\n",
            "Train Epoch: 6 [640/1093 (59%)]\tLoss: 2.918444\n",
            "Train Epoch: 6 [660/1093 (60%)]\tLoss: 1.606278\n",
            "Train Epoch: 6 [680/1093 (62%)]\tLoss: 2.919009\n",
            "Train Epoch: 6 [700/1093 (64%)]\tLoss: 1.834512\n",
            "Train Epoch: 6 [720/1093 (66%)]\tLoss: 1.128556\n",
            "Train Epoch: 6 [740/1093 (68%)]\tLoss: 0.458304\n",
            "Train Epoch: 6 [760/1093 (70%)]\tLoss: 2.288797\n",
            "Train Epoch: 6 [780/1093 (71%)]\tLoss: 0.474955\n",
            "Train Epoch: 6 [800/1093 (73%)]\tLoss: 0.831960\n",
            "Train Epoch: 6 [820/1093 (75%)]\tLoss: 2.240485\n",
            "Train Epoch: 6 [840/1093 (77%)]\tLoss: 1.102492\n",
            "Train Epoch: 6 [860/1093 (79%)]\tLoss: 1.705472\n",
            "Train Epoch: 6 [880/1093 (81%)]\tLoss: 1.797313\n",
            "Train Epoch: 6 [900/1093 (82%)]\tLoss: 0.431705\n",
            "Train Epoch: 6 [920/1093 (84%)]\tLoss: 1.253347\n",
            "Train Epoch: 6 [940/1093 (86%)]\tLoss: 2.190354\n",
            "Train Epoch: 6 [960/1093 (88%)]\tLoss: 1.054296\n",
            "Train Epoch: 6 [980/1093 (90%)]\tLoss: 0.745628\n",
            "Train Epoch: 6 [1000/1093 (92%)]\tLoss: 2.065915\n",
            "Train Epoch: 6 [1020/1093 (93%)]\tLoss: 0.740503\n",
            "Train Epoch: 6 [1040/1093 (95%)]\tLoss: 1.334283\n",
            "Train Epoch: 6 [1060/1093 (97%)]\tLoss: 1.461951\n",
            "Train Epoch: 6 [1080/1093 (99%)]\tLoss: 0.498094\n",
            "-------- Validating ... --------\n",
            "Current Acc: 51.28 %\n",
            "Best Acc: 51.28 %\n",
            "Best Epoch:  6\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 7 [0/1093 (0%)]\tLoss: 1.509883\n",
            "Train Epoch: 7 [20/1093 (2%)]\tLoss: 2.751319\n",
            "Train Epoch: 7 [40/1093 (4%)]\tLoss: 2.240294\n",
            "Train Epoch: 7 [60/1093 (5%)]\tLoss: 2.965020\n",
            "Train Epoch: 7 [80/1093 (7%)]\tLoss: 1.600340\n",
            "Train Epoch: 7 [100/1093 (9%)]\tLoss: 1.606217\n",
            "Train Epoch: 7 [120/1093 (11%)]\tLoss: 1.686331\n",
            "Train Epoch: 7 [140/1093 (13%)]\tLoss: 1.277451\n",
            "Train Epoch: 7 [160/1093 (15%)]\tLoss: 0.786566\n",
            "Train Epoch: 7 [180/1093 (16%)]\tLoss: 0.711147\n",
            "Train Epoch: 7 [200/1093 (18%)]\tLoss: 1.745892\n",
            "Train Epoch: 7 [220/1093 (20%)]\tLoss: 0.507504\n",
            "Train Epoch: 7 [240/1093 (22%)]\tLoss: 1.648518\n",
            "Train Epoch: 7 [260/1093 (24%)]\tLoss: 0.735357\n",
            "Train Epoch: 7 [280/1093 (26%)]\tLoss: 0.938360\n",
            "Train Epoch: 7 [300/1093 (27%)]\tLoss: 1.730920\n",
            "Train Epoch: 7 [320/1093 (29%)]\tLoss: 1.667403\n",
            "Train Epoch: 7 [340/1093 (31%)]\tLoss: 1.205708\n",
            "Train Epoch: 7 [360/1093 (33%)]\tLoss: 0.765769\n",
            "Train Epoch: 7 [380/1093 (35%)]\tLoss: 1.734252\n",
            "Train Epoch: 7 [400/1093 (37%)]\tLoss: 2.113629\n",
            "Train Epoch: 7 [420/1093 (38%)]\tLoss: 1.820647\n",
            "Train Epoch: 7 [440/1093 (40%)]\tLoss: 2.355449\n",
            "Train Epoch: 7 [460/1093 (42%)]\tLoss: 1.999455\n",
            "Train Epoch: 7 [480/1093 (44%)]\tLoss: 0.771810\n",
            "Train Epoch: 7 [500/1093 (46%)]\tLoss: 1.929632\n",
            "Train Epoch: 7 [520/1093 (48%)]\tLoss: 2.163139\n",
            "Train Epoch: 7 [540/1093 (49%)]\tLoss: 1.003671\n",
            "Train Epoch: 7 [560/1093 (51%)]\tLoss: 0.427752\n",
            "Train Epoch: 7 [580/1093 (53%)]\tLoss: 0.862471\n",
            "Train Epoch: 7 [600/1093 (55%)]\tLoss: 1.397638\n",
            "Train Epoch: 7 [620/1093 (57%)]\tLoss: 1.394957\n",
            "Train Epoch: 7 [640/1093 (59%)]\tLoss: 2.044497\n",
            "Train Epoch: 7 [660/1093 (60%)]\tLoss: 0.546612\n",
            "Train Epoch: 7 [680/1093 (62%)]\tLoss: 1.133477\n",
            "Train Epoch: 7 [700/1093 (64%)]\tLoss: 2.107524\n",
            "Train Epoch: 7 [720/1093 (66%)]\tLoss: 1.119154\n",
            "Train Epoch: 7 [740/1093 (68%)]\tLoss: 1.637713\n",
            "Train Epoch: 7 [760/1093 (70%)]\tLoss: 1.176902\n",
            "Train Epoch: 7 [780/1093 (71%)]\tLoss: 1.181175\n",
            "Train Epoch: 7 [800/1093 (73%)]\tLoss: 1.479626\n",
            "Train Epoch: 7 [820/1093 (75%)]\tLoss: 1.700113\n",
            "Train Epoch: 7 [840/1093 (77%)]\tLoss: 0.652163\n",
            "Train Epoch: 7 [860/1093 (79%)]\tLoss: 0.840134\n",
            "Train Epoch: 7 [880/1093 (81%)]\tLoss: 2.015305\n",
            "Train Epoch: 7 [900/1093 (82%)]\tLoss: 1.826678\n",
            "Train Epoch: 7 [920/1093 (84%)]\tLoss: 1.539371\n",
            "Train Epoch: 7 [940/1093 (86%)]\tLoss: 0.866538\n",
            "Train Epoch: 7 [960/1093 (88%)]\tLoss: 2.169232\n",
            "Train Epoch: 7 [980/1093 (90%)]\tLoss: 1.291516\n",
            "Train Epoch: 7 [1000/1093 (92%)]\tLoss: 1.852942\n",
            "Train Epoch: 7 [1020/1093 (93%)]\tLoss: 2.523578\n",
            "Train Epoch: 7 [1040/1093 (95%)]\tLoss: 1.969038\n",
            "Train Epoch: 7 [1060/1093 (97%)]\tLoss: 2.149312\n",
            "Train Epoch: 7 [1080/1093 (99%)]\tLoss: 1.173068\n",
            "-------- Validating ... --------\n",
            "Current Acc: 48.08 %\n",
            "Best Acc: 51.28 %\n",
            "Best Epoch:  6\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 8 [0/1093 (0%)]\tLoss: 1.120949\n",
            "Train Epoch: 8 [20/1093 (2%)]\tLoss: 1.231064\n",
            "Train Epoch: 8 [40/1093 (4%)]\tLoss: 2.322587\n",
            "Train Epoch: 8 [60/1093 (5%)]\tLoss: 1.663010\n",
            "Train Epoch: 8 [80/1093 (7%)]\tLoss: 0.969980\n",
            "Train Epoch: 8 [100/1093 (9%)]\tLoss: 0.856467\n",
            "Train Epoch: 8 [120/1093 (11%)]\tLoss: 0.531085\n",
            "Train Epoch: 8 [140/1093 (13%)]\tLoss: 1.220412\n",
            "Train Epoch: 8 [160/1093 (15%)]\tLoss: 1.188401\n",
            "Train Epoch: 8 [180/1093 (16%)]\tLoss: 1.280704\n",
            "Train Epoch: 8 [200/1093 (18%)]\tLoss: 1.215605\n",
            "Train Epoch: 8 [220/1093 (20%)]\tLoss: 1.402242\n",
            "Train Epoch: 8 [240/1093 (22%)]\tLoss: 1.144164\n",
            "Train Epoch: 8 [260/1093 (24%)]\tLoss: 0.833558\n",
            "Train Epoch: 8 [280/1093 (26%)]\tLoss: 1.013538\n",
            "Train Epoch: 8 [300/1093 (27%)]\tLoss: 1.384091\n",
            "Train Epoch: 8 [320/1093 (29%)]\tLoss: 1.147892\n",
            "Train Epoch: 8 [340/1093 (31%)]\tLoss: 0.452065\n",
            "Train Epoch: 8 [360/1093 (33%)]\tLoss: 1.656914\n",
            "Train Epoch: 8 [380/1093 (35%)]\tLoss: 1.731417\n",
            "Train Epoch: 8 [400/1093 (37%)]\tLoss: 2.254428\n",
            "Train Epoch: 8 [420/1093 (38%)]\tLoss: 2.100357\n",
            "Train Epoch: 8 [440/1093 (40%)]\tLoss: 1.321013\n",
            "Train Epoch: 8 [460/1093 (42%)]\tLoss: 1.798867\n",
            "Train Epoch: 8 [480/1093 (44%)]\tLoss: 2.107893\n",
            "Train Epoch: 8 [500/1093 (46%)]\tLoss: 2.356095\n",
            "Train Epoch: 8 [520/1093 (48%)]\tLoss: 1.938165\n",
            "Train Epoch: 8 [540/1093 (49%)]\tLoss: 1.455754\n",
            "Train Epoch: 8 [560/1093 (51%)]\tLoss: 1.549762\n",
            "Train Epoch: 8 [580/1093 (53%)]\tLoss: 0.977449\n",
            "Train Epoch: 8 [600/1093 (55%)]\tLoss: 0.926804\n",
            "Train Epoch: 8 [620/1093 (57%)]\tLoss: 0.670936\n",
            "Train Epoch: 8 [640/1093 (59%)]\tLoss: 1.102693\n",
            "Train Epoch: 8 [660/1093 (60%)]\tLoss: 1.318989\n",
            "Train Epoch: 8 [680/1093 (62%)]\tLoss: 1.388334\n",
            "Train Epoch: 8 [700/1093 (64%)]\tLoss: 2.770322\n",
            "Train Epoch: 8 [720/1093 (66%)]\tLoss: 0.984812\n",
            "Train Epoch: 8 [740/1093 (68%)]\tLoss: 0.967265\n",
            "Train Epoch: 8 [760/1093 (70%)]\tLoss: 1.148122\n",
            "Train Epoch: 8 [780/1093 (71%)]\tLoss: 0.998778\n",
            "Train Epoch: 8 [800/1093 (73%)]\tLoss: 2.014994\n",
            "Train Epoch: 8 [820/1093 (75%)]\tLoss: 1.619998\n",
            "Train Epoch: 8 [840/1093 (77%)]\tLoss: 1.983283\n",
            "Train Epoch: 8 [860/1093 (79%)]\tLoss: 1.653548\n",
            "Train Epoch: 8 [880/1093 (81%)]\tLoss: 1.548484\n",
            "Train Epoch: 8 [900/1093 (82%)]\tLoss: 0.974042\n",
            "Train Epoch: 8 [920/1093 (84%)]\tLoss: 3.162122\n",
            "Train Epoch: 8 [940/1093 (86%)]\tLoss: 2.620270\n",
            "Train Epoch: 8 [960/1093 (88%)]\tLoss: 1.786994\n",
            "Train Epoch: 8 [980/1093 (90%)]\tLoss: 0.863108\n",
            "Train Epoch: 8 [1000/1093 (92%)]\tLoss: 2.075640\n",
            "Train Epoch: 8 [1020/1093 (93%)]\tLoss: 1.445294\n",
            "Train Epoch: 8 [1040/1093 (95%)]\tLoss: 0.762194\n",
            "Train Epoch: 8 [1060/1093 (97%)]\tLoss: 0.804217\n",
            "Train Epoch: 8 [1080/1093 (99%)]\tLoss: 0.955153\n",
            "-------- Validating ... --------\n",
            "Current Acc: 51.28 %\n",
            "Best Acc: 51.28 %\n",
            "Best Epoch:  8\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 9 [0/1093 (0%)]\tLoss: 1.798567\n",
            "Train Epoch: 9 [20/1093 (2%)]\tLoss: 1.033601\n",
            "Train Epoch: 9 [40/1093 (4%)]\tLoss: 0.896524\n",
            "Train Epoch: 9 [60/1093 (5%)]\tLoss: 1.434328\n",
            "Train Epoch: 9 [80/1093 (7%)]\tLoss: 0.963121\n",
            "Train Epoch: 9 [100/1093 (9%)]\tLoss: 0.988312\n",
            "Train Epoch: 9 [120/1093 (11%)]\tLoss: 0.713024\n",
            "Train Epoch: 9 [140/1093 (13%)]\tLoss: 1.295399\n",
            "Train Epoch: 9 [160/1093 (15%)]\tLoss: 2.655236\n",
            "Train Epoch: 9 [180/1093 (16%)]\tLoss: 1.714552\n",
            "Train Epoch: 9 [200/1093 (18%)]\tLoss: 0.323064\n",
            "Train Epoch: 9 [220/1093 (20%)]\tLoss: 2.002909\n",
            "Train Epoch: 9 [240/1093 (22%)]\tLoss: 0.442648\n",
            "Train Epoch: 9 [260/1093 (24%)]\tLoss: 1.131707\n",
            "Train Epoch: 9 [280/1093 (26%)]\tLoss: 1.067430\n",
            "Train Epoch: 9 [300/1093 (27%)]\tLoss: 0.520323\n",
            "Train Epoch: 9 [320/1093 (29%)]\tLoss: 1.121254\n",
            "Train Epoch: 9 [340/1093 (31%)]\tLoss: 1.002206\n",
            "Train Epoch: 9 [360/1093 (33%)]\tLoss: 0.506572\n",
            "Train Epoch: 9 [380/1093 (35%)]\tLoss: 0.599978\n",
            "Train Epoch: 9 [400/1093 (37%)]\tLoss: 0.877806\n",
            "Train Epoch: 9 [420/1093 (38%)]\tLoss: 1.173052\n",
            "Train Epoch: 9 [440/1093 (40%)]\tLoss: 1.238909\n",
            "Train Epoch: 9 [460/1093 (42%)]\tLoss: 1.538719\n",
            "Train Epoch: 9 [480/1093 (44%)]\tLoss: 1.519647\n",
            "Train Epoch: 9 [500/1093 (46%)]\tLoss: 1.739803\n",
            "Train Epoch: 9 [520/1093 (48%)]\tLoss: 1.192060\n",
            "Train Epoch: 9 [540/1093 (49%)]\tLoss: 2.119545\n",
            "Train Epoch: 9 [560/1093 (51%)]\tLoss: 2.031641\n",
            "Train Epoch: 9 [580/1093 (53%)]\tLoss: 0.832931\n",
            "Train Epoch: 9 [600/1093 (55%)]\tLoss: 0.991265\n",
            "Train Epoch: 9 [620/1093 (57%)]\tLoss: 1.211863\n",
            "Train Epoch: 9 [640/1093 (59%)]\tLoss: 1.082829\n",
            "Train Epoch: 9 [660/1093 (60%)]\tLoss: 0.717184\n",
            "Train Epoch: 9 [680/1093 (62%)]\tLoss: 0.500792\n",
            "Train Epoch: 9 [700/1093 (64%)]\tLoss: 2.048022\n",
            "Train Epoch: 9 [720/1093 (66%)]\tLoss: 0.823059\n",
            "Train Epoch: 9 [740/1093 (68%)]\tLoss: 1.121972\n",
            "Train Epoch: 9 [760/1093 (70%)]\tLoss: 1.707786\n",
            "Train Epoch: 9 [780/1093 (71%)]\tLoss: 0.285379\n",
            "Train Epoch: 9 [800/1093 (73%)]\tLoss: 1.452637\n",
            "Train Epoch: 9 [820/1093 (75%)]\tLoss: 1.003258\n",
            "Train Epoch: 9 [840/1093 (77%)]\tLoss: 0.655006\n",
            "Train Epoch: 9 [860/1093 (79%)]\tLoss: 1.836505\n",
            "Train Epoch: 9 [880/1093 (81%)]\tLoss: 0.656521\n",
            "Train Epoch: 9 [900/1093 (82%)]\tLoss: 1.603774\n",
            "Train Epoch: 9 [920/1093 (84%)]\tLoss: 0.809420\n",
            "Train Epoch: 9 [940/1093 (86%)]\tLoss: 2.570333\n",
            "Train Epoch: 9 [960/1093 (88%)]\tLoss: 1.677709\n",
            "Train Epoch: 9 [980/1093 (90%)]\tLoss: 1.756782\n",
            "Train Epoch: 9 [1000/1093 (92%)]\tLoss: 0.822049\n",
            "Train Epoch: 9 [1020/1093 (93%)]\tLoss: 1.455905\n",
            "Train Epoch: 9 [1040/1093 (95%)]\tLoss: 0.741139\n",
            "Train Epoch: 9 [1060/1093 (97%)]\tLoss: 0.851923\n",
            "Train Epoch: 9 [1080/1093 (99%)]\tLoss: 0.534624\n",
            "-------- Validating ... --------\n",
            "Current Acc: 52.56 %\n",
            "Best Acc: 52.56 %\n",
            "Best Epoch:  9\n",
            "********************\n",
            "-------- Training ... --------\n",
            "Train Epoch: 10 [0/1093 (0%)]\tLoss: 0.799409\n",
            "Train Epoch: 10 [20/1093 (2%)]\tLoss: 0.767976\n",
            "Train Epoch: 10 [40/1093 (4%)]\tLoss: 0.796722\n",
            "Train Epoch: 10 [60/1093 (5%)]\tLoss: 1.209935\n",
            "Train Epoch: 10 [80/1093 (7%)]\tLoss: 2.047330\n",
            "Train Epoch: 10 [100/1093 (9%)]\tLoss: 2.307642\n",
            "Train Epoch: 10 [120/1093 (11%)]\tLoss: 1.289784\n",
            "Train Epoch: 10 [140/1093 (13%)]\tLoss: 0.432715\n",
            "Train Epoch: 10 [160/1093 (15%)]\tLoss: 2.238145\n",
            "Train Epoch: 10 [180/1093 (16%)]\tLoss: 1.069733\n",
            "Train Epoch: 10 [200/1093 (18%)]\tLoss: 2.196664\n",
            "Train Epoch: 10 [220/1093 (20%)]\tLoss: 1.882675\n",
            "Train Epoch: 10 [240/1093 (22%)]\tLoss: 1.309681\n",
            "Train Epoch: 10 [260/1093 (24%)]\tLoss: 0.611594\n",
            "Train Epoch: 10 [280/1093 (26%)]\tLoss: 2.253329\n",
            "Train Epoch: 10 [300/1093 (27%)]\tLoss: 0.664312\n",
            "Train Epoch: 10 [320/1093 (29%)]\tLoss: 1.291902\n",
            "Train Epoch: 10 [340/1093 (31%)]\tLoss: 1.854808\n",
            "Train Epoch: 10 [360/1093 (33%)]\tLoss: 0.694571\n",
            "Train Epoch: 10 [380/1093 (35%)]\tLoss: 1.229950\n",
            "Train Epoch: 10 [400/1093 (37%)]\tLoss: 1.857698\n",
            "Train Epoch: 10 [420/1093 (38%)]\tLoss: 1.608074\n",
            "Train Epoch: 10 [440/1093 (40%)]\tLoss: 0.699402\n",
            "Train Epoch: 10 [460/1093 (42%)]\tLoss: 0.644485\n",
            "Train Epoch: 10 [480/1093 (44%)]\tLoss: 1.094102\n",
            "Train Epoch: 10 [500/1093 (46%)]\tLoss: 0.909184\n",
            "Train Epoch: 10 [520/1093 (48%)]\tLoss: 1.845256\n",
            "Train Epoch: 10 [540/1093 (49%)]\tLoss: 1.519351\n",
            "Train Epoch: 10 [560/1093 (51%)]\tLoss: 0.471530\n",
            "Train Epoch: 10 [580/1093 (53%)]\tLoss: 1.283155\n",
            "Train Epoch: 10 [600/1093 (55%)]\tLoss: 0.660176\n",
            "Train Epoch: 10 [620/1093 (57%)]\tLoss: 1.120650\n",
            "Train Epoch: 10 [640/1093 (59%)]\tLoss: 1.477569\n",
            "Train Epoch: 10 [660/1093 (60%)]\tLoss: 1.036175\n",
            "Train Epoch: 10 [680/1093 (62%)]\tLoss: 1.608216\n",
            "Train Epoch: 10 [700/1093 (64%)]\tLoss: 1.061806\n",
            "Train Epoch: 10 [720/1093 (66%)]\tLoss: 1.539900\n",
            "Train Epoch: 10 [740/1093 (68%)]\tLoss: 1.562034\n",
            "Train Epoch: 10 [760/1093 (70%)]\tLoss: 1.335517\n",
            "Train Epoch: 10 [780/1093 (71%)]\tLoss: 0.747334\n",
            "Train Epoch: 10 [800/1093 (73%)]\tLoss: 0.956299\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV8HsArDk734"
      },
      "source": [
        "## 7. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S0yGUqwlCjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0183047b-26e2-4b77-d78f-c08ffb29f1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------- Spatio-temporal Reasoning Network(PSTP-Net) --------------- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import ast\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"\\n--------------- Spatio-temporal Reasoning Network(PSTP-Net) --------------- \\n\")\n",
        "\n",
        "def test(model, val_loader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    samples = json.load(open(args['label_test'], 'r'))\n",
        "\n",
        "    # prediction save\n",
        "    A_count = []\n",
        "    A_compt = []\n",
        "    V_count = []\n",
        "    V_local = []\n",
        "    AV_exist = []\n",
        "    AV_count = []\n",
        "    AV_local = []\n",
        "    AV_compt = []\n",
        "    AV_templ = []\n",
        "\n",
        "    # results save\n",
        "    que_id = []\n",
        "    pred_results =[]\n",
        "    grd_target = []\n",
        "    pred_label = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, sample in enumerate(val_loader):\n",
        "\n",
        "            name, audio_feat, visual_feat, question_feat, target, sg_data, qg_data, question_id = sample\n",
        "            if sg_data[0].x.shape == (0, 512):\n",
        "                print(batch_idx)\n",
        "            qg_data = qg_data.to('cuda')\n",
        "            for i in range(len(sg_data)):\n",
        "                sg_data[i] = sg_data[i].to('cuda')\n",
        "            audio_feat = audio_feat.to('cuda')\n",
        "            visual_feat = visual_feat.to('cuda')\n",
        "            question_feat = question_feat.to('cuda')\n",
        "            target = target.to('cuda')\n",
        "\n",
        "            preds_qa = model(audio_feat, visual_feat, question_feat, sg_data, qg_data)\n",
        "\n",
        "            preds = preds_qa\n",
        "\n",
        "            _, predicted = torch.max(preds.data, 1)\n",
        "            # print(preds.data, predicted, target)\n",
        "\n",
        "            total += preds.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            # result\n",
        "            grd_target.append(target.cpu().item())\n",
        "            pred_label.append(predicted.cpu().item())\n",
        "\n",
        "            pred_bool = predicted == target\n",
        "            for index in range(len(pred_bool)):\n",
        "                pred_results.append(pred_bool[index].cpu().item())\n",
        "                que_id.append(question_id[index].item())\n",
        "\n",
        "\n",
        "            x = samples[batch_idx]\n",
        "            type =ast.literal_eval(x['type'])\n",
        "            if type[0] == 'Audio':\n",
        "                if type[1] == 'Counting':\n",
        "                    A_count.append((predicted == target).sum().item())\n",
        "                elif type[1] == 'Comparative':\n",
        "                    A_compt.append((predicted == target).sum().item())\n",
        "            elif type[0] == 'Visual':\n",
        "                if type[1] == 'Counting':\n",
        "                    V_count.append((predicted == target).sum().item())\n",
        "                elif type[1] == 'Location':\n",
        "                    V_local.append((predicted == target).sum().item())\n",
        "            elif type[0] == 'Audio-Visual':\n",
        "                if type[1] == 'Existential':\n",
        "                    AV_exist.append((predicted == target).sum().item())\n",
        "                elif type[1] == 'Counting':\n",
        "                    AV_count.append((predicted == target).sum().item())\n",
        "                elif type[1] == 'Location':\n",
        "                    AV_local.append((predicted == target).sum().item())\n",
        "                elif type[1] == 'Comparative':\n",
        "                    AV_compt.append((predicted == target).sum().item())\n",
        "                elif type[1] == 'Temporal':\n",
        "                    AV_templ.append((predicted == target).sum().item())\n",
        "\n",
        "    print('\\nAudio Count Acc: %.2f %%' % (100 * sum(A_count)/len(A_count)))\n",
        "    print('Audio Compt Acc: %.2f %%' % (100 * sum(A_compt) / len(A_compt)))\n",
        "    print('Audio Averg Acc: %.2f %%' % (100 * (sum(A_count) + sum(A_compt)) / (len(A_count) + len(A_compt))))\n",
        "\n",
        "    print('\\nVisual Count Acc: %.2f %%' % (100 * sum(V_count) / len(V_count)))\n",
        "    print('Visual Local Acc: %.2f %%' % (100 * sum(V_local) / len(V_local)))\n",
        "    print('Visual Averg Acc: %.2f %%' % (100 * (sum(V_count) + sum(V_local)) / (len(V_count) + len(V_local))))\n",
        "\n",
        "    print('\\nAudio-Visual Exist Acc: %.2f %%' % (100 * sum(AV_exist) / len(AV_exist)))\n",
        "    print('Audio-Visual Count Acc: %.2f %%' % (100 * sum(AV_count) / len(AV_count)))\n",
        "    print('Audio-Visual Local Acc: %.2f %%' % (100 * sum(AV_local) / len(AV_local)))\n",
        "    print('Audio-Visual Compt Acc: %.2f %%' % (100 * sum(AV_compt) / len(AV_compt)))\n",
        "    print('Audio-Visual Templ Acc: %.2f %%' % (100 * sum(AV_templ) / len(AV_templ)))\n",
        "    print('Audio-Visual Averg Acc: %.2f %%' % (100 * (sum(AV_count) + sum(AV_local) + sum(AV_exist) + sum(AV_templ) + sum(AV_compt)) /\n",
        "                                                     (len(AV_count) + len(AV_local) + len(AV_exist) + len(AV_templ) + len(AV_compt))))\n",
        "\n",
        "    print('\\n---->Overall Accuracy: %.2f %%' % (100 * correct / total), \"\\n\")\n",
        "\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    with open(\"results/AVQA-GNN_Net.txt\", 'w') as f:\n",
        "        for index in range(len(que_id)):\n",
        "            # print(que_id[index],' \\t ',pred_results[index],' \\t ',grd_target[index],' \\t ',pred_label[index])\n",
        "            f.write(str(que_id[index])+' \\t '+str(pred_results[index])+' \\t '+str(grd_target[index])+' \\t '+str(pred_label[index])+'\\n')\n",
        "\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "\n",
        "def main_test():\n",
        "\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args['gpu']\n",
        "\n",
        "    torch.manual_seed(args['seed'])\n",
        "\n",
        "    model = AVQA_GNN(args)\n",
        "    model = nn.DataParallel(model)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    test_dataset = AVQA_dataset(args = args,\n",
        "                                label = args['label_test'],\n",
        "                                audios_feat_dir = args['audios_feat_dir'],\n",
        "                                clip_vit_b32_dir = args['clip_vit_b32_dir'],\n",
        "                                clip_qst_dir = args['clip_qst_dir'])\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=args['num_workers'], collate_fn=AVQA_dataset_collate_fn)\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(args['model_save_dir'], args['checkpoint'] + \".pt\")))\n",
        "    test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyhoK-cnC1VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86220b8-35c5-4977-810d-627728aa9ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "<ipython-input-8-c3e10dd018ee>:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  a_ij = F.softmax(torch.bmm(self.w(video_conv), audio_conv.permute(0, 2, 1)) / torch.sqrt(torch.tensor(video_conv.shape[-1]))) # [4, 10, 10]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Audio Count Acc: 54.17 %\n",
            "Audio Compt Acc: 47.22 %\n",
            "Audio Averg Acc: 50.00 %\n",
            "\n",
            "Visual Count Acc: 65.52 %\n",
            "Visual Local Acc: 30.00 %\n",
            "Visual Averg Acc: 47.46 %\n",
            "\n",
            "Audio-Visual Exist Acc: 59.46 %\n",
            "Audio-Visual Count Acc: 50.00 %\n",
            "Audio-Visual Local Acc: 35.00 %\n",
            "Audio-Visual Compt Acc: 56.76 %\n",
            "Audio-Visual Templ Acc: 20.69 %\n",
            "Audio-Visual Averg Acc: 45.64 %\n",
            "\n",
            "---->Overall Accuracy: 46.82 % \n",
            "\n"
          ]
        }
      ],
      "source": [
        "main_test()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "75-1kAqf7JGq",
        "h4500WAwhqHg"
      ],
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}